{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- Section 1: Introduction\n",
    "- Section 2: Methodology\n",
    "    - Section 2.i: Obtain Data\n",
    "        - Section 2.i.a: Breath Tables\n",
    "        - Section 2.i.b: Tidal breathing flow-loop loop (TBFVL) Table\n",
    "        - Section 2.i.c: SPX Table\n",
    "        - Section 2.i.d: Screenshot Table\n",
    "    - Section 2.ii: Organize Data\n",
    "        - Section 2.ii.a: Clean and combine REDCap\n",
    "        - Section 2.ii.b: Link raw data\n",
    "        - Section 2.ii.c: Split data set\n",
    "    - Section 2.iii: Preprocess Data\n",
    "    - Section 2.iv: Load and Process Data\n",
    "    - Section 2.v: Model Training\n",
    "- Section 3: Results\n",
    "- Section 4: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction<a name=\"section-1-introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current spirometry values, such forced expiratory volume in 1 second (FEV<sub>1</sub>), are used to quantitatively assess the disease progression of restrictive diseases such as pulmonary fibrosis. FEV<sub>1</sub> is best suited for detecting changes in large airways and may not be sensitive enough to adequately detect peripheral changes in other lung diseases such as early stage cystic fibrosis.\n",
    "\n",
    "The lung clearance index (LCI), a ratio of the cumulative expired volume to the functional residual capacity, has been proposed as an alternative metric to the current spirometry paradigm. The LCI can be thought of as the number of times the resting breath volume has to be turned over in order to remove a tracer gas from the lungs. If an individual has ventilation inhomogeneity due to lung obstructions, such as those associated with cystic fibrosis, a higher number of breaths and associated volume will be required to remove the tracer gas from the lung.\n",
    "\n",
    "The LCI is obtained through the [multiple breath inert gas washout (MBW)](http://www.mbwtraining.com/ECFS_MBW_SOP.pdf). Currently, the LCI is a research metric and not a clinical metric. Industry research studies utilizing MBW usually involve a third party to review the MBW trials to determine if the MBW trial is research quality. The North American, European, and Australian MBW central over-reading centres (CORCs) exist to formally review the MBW data. The raw MBW signals (Figure 1; CO<sub>2</sub>, O<sub>2</sub>, N<sub>2</sub>, flow, volume) are quality controlled for the following critera:\n",
    "- No leaks\n",
    "- No coughing, laughing or talking\n",
    "- No signal misalignment\n",
    "- Adequate time between trials\n",
    "- Clear end of test\n",
    "- No abnormal tidal breathing patterns    \n",
    "\n",
    "For a more complete discussion of the criteria of a success trial, please refer to the [Central Overreading Centre's training documentation](https://lab.research.sickkids.ca/ratjen/wp-content/uploads/sites/39/2017/09/MBWN2-Training-and-Qualification-Requirements_Aug-30-2017.pdf). There is potential for automation since reviewing MBW data can be labour intensive. The primary purpose of this project was to classify trial outcome ('accept' or 'reject') with trial grades ('A', 'B', 'C', 'D', 'E', 'F', 'N/A') as a secondary outcome, where a trial grade of 'A', 'B', or 'C' corresponds to an 'accept' outcome.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/mbw_example.png\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  Figure 1. Example of raw multiple breath inert gas washout signals. From top to bottom, CO<sub>2</sub>, O<sub>2</sub>, N<sub>2</sub>, flow, volume.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Methodology<a name=\"section-2-methodology\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.i: Obtain Data<a name=\"section-2i-data\"></a>\n",
    "Data for this project was derived from two studies involving MBW: TRACK, and LONGITUDINAL. As part of those studies, data was collected and processed in Spiroware version 3.2 or earlier with each individual trial being given a grade and an outcome ('accept' or 'reject') by trained CORC over-readers. The end goal of this section is to describe how the data for this project was obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.i.a: Breath and Tidal breathing flow-volume loop (TBFVL) Tables<a name=\"section-2ia-breath-tbfvl\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Breath Tables provide summary information associated with a single breath such as inspiratory and expiratory volume. The TBFVL derives metrics, such as respiratory quotient, using a flow volume loop during tidal breathing. The Breath and TBFVL tables were previously obtained from the Spiroware software as part of the original study analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.i.b: SPX tables<a name=\"section-2ib-spx-tables\"></a>\n",
    "Like the Breath and TBFVL tables, the SPX tables were obtained from the Spiroware software. Unlike the Breath and TBFVL tables, the SPX tables contained summary information obtained at the conclusion of the trial (i.e. LCI). In contrast, the Breath and TBFVL tables report data collected during the trial. Because of how the data was exported, the raw SPX files contains data from multiple subjects and trials. The raw SPX data needed to be split so a single subject-trial combination was contained in a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL_EXPORT_PATH = \"../data/external/Primary Training Data Set/SPX Exports/\"\n",
    "SPX_PATH = \"../data/raw/spx_exports/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, _, files in os.walk(TRIAL_EXPORT_PATH):\n",
    "    for file in files:\n",
    "        # read data file with combined subject, trials\n",
    "        if file.endswith('N2MultiBreathWashoutTest_TrialData.csv'):\n",
    "            # all files have a semi-colon separator except for 'IND 241-259'\n",
    "            if 'IND 241-259' in os.path.join(root, file):\n",
    "                spx_export = pd.read_csv(os.path.join(root, file))\n",
    "            else:\n",
    "                spx_export = pd.read_csv(\n",
    "                    os.path.join(root, file), sep=';', skiprows=1\n",
    "                )\n",
    "            \n",
    "            # split into individual subject-trial combination and save\n",
    "            for patient_trial in zip(\n",
    "                spx_export['Patient-ID'], spx_export['Trial #']\n",
    "            ):\n",
    "                patient, trial = patient_trial\n",
    "\n",
    "                patient_trial_spx = spx_export[\n",
    "                    (spx_export['Patient-ID'] == patient)\n",
    "                    & ((spx_export['Trial #'] == trial)) \n",
    "                ]\n",
    "\n",
    "                # quality check to determine if data is missing\n",
    "                if patient_trial_spx.shape[0] == 1:\n",
    "                    patient_trial_spx.to_csv(\n",
    "                        '{}{}_trial_{}_spx.csv'.format(\n",
    "                            SPX_PATH, str(patient), str(trial)\n",
    "                        ),\n",
    "                        index=False\n",
    "                    )\n",
    "                else:\n",
    "                    print(patient_trial_spx.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.i.c: Screenshot tables<a name=\"section-2ic-screenshot-tables\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw spirometry signals (N2, O2, CO2, volume, flow) is effectively the same as the Breath and TBFVL data but with a higher granularity. Obtaining the data from the raw spirometry signals was done in three steps:\n",
    "1. Taking screenshots of the raw spirometry figure in Spiroware  \n",
    "    An image of the Spiroware data was collected using the script `mbw_qc/mbw_qc/data/1-spiroware_screenshot.py`. An example of the script in operation is given in the video below.\n",
    "2. Confirm screenshots integrity  \n",
    "    A second python script, `mbw_qc/mbw_qc/data/2-confirm_screenshot.py`, was run on the captured screenshots to checking the integrity of the screenshots. The script worked by confirming the location and wording of the vertical axis. Any issues found with this script resulting in rerunning the `mbw_qc/mbw_qc/data/1-spiroware_screenshot.py` script on the trials with issues.\n",
    "3. Digitize the screenshots  \n",
    "    If no additional issues were identified, the script `mbw_qc/mbw_qc/data/3-digitize_screenshot.py` was used to convert the screenshot to numeric values. The script uses a modified version of [plotdigitizer](https://pypi.org/project/plotdigitizer/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <video width='640' height='480' controls src='../notebooks/assets/spiroware_screenshots.mp4'>animation</video>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the screenshots were digitized, outlier values were checked to determine if there were any oustanding issues with the previous two scripts. Because of the nature of the task, it was not possible to test the functions with a testing suite such as `pytest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw/digitize_screenshots\\216.6_trial_4_o2.csv\n",
      "../data/raw/digitize_screenshots\\241_25_trial_3_o2.csv\n",
      "../data/raw/digitize_screenshots\\245_29_trial_4_o2.csv\n",
      "../data/raw/digitize_screenshots\\313.2_trial_1_o2.csv\n",
      "../data/raw/digitize_screenshots\\LSC02_22_trial_3_o2.csv\n",
      "../data/raw/digitize_screenshots\\LSC02_28_trial_5_o2.csv\n",
      "../data/raw/digitize_screenshots\\LSC46_25_trial_1_o2.csv\n",
      "../data/raw/digitize_screenshots\\LSC49_26_trial_1_o2.csv\n",
      "../data/raw/digitize_screenshots\\LSC59_29_trial_1_o2.csv\n",
      "../data/raw/digitize_screenshots\\LSC68_26_trial_3_o2.csv\n",
      "../data/raw/digitize_screenshots\\203_24_trial_6_n2.csv\n",
      "../data/raw/digitize_screenshots\\205_24_trial_3_n2.csv\n",
      "../data/raw/digitize_screenshots\\205_30_trial_2_n2.csv\n",
      "../data/raw/digitize_screenshots\\206_21_trial_2_n2.csv\n",
      "../data/raw/digitize_screenshots\\231_22_trial_6_n2.csv\n",
      "../data/raw/digitize_screenshots\\242_25_trial_5_n2.csv\n",
      "../data/raw/digitize_screenshots\\245_29_trial_4_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC02_22_trial_3_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC02_28_trial_5_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC03_30_trial_6_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC09_27_trial_1_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC12_29_trial_4_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC18_23_trial_2_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC18_23_trial_6_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_22_trial_2_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_22_trial_4_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_30_trial_3_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC43_28_trial_3_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC45_21_trial_2_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC45_21_trial_3_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC45_23_trial_1_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC49_26_trial_1_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC49_26_trial_2_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC49_27_trial_3_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC49_28_trial_1_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_30_trial_2_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC56_25_trial_4_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC59_29_trial_1_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC59_29_trial_3_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC59_29_trial_5_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC61_22_trial_4_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC61_31_trial_5_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC68_26_trial_3_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC77_21_trial_2_n2.csv\n",
      "../data/raw/digitize_screenshots\\LSC78_27_trial_2_n2.csv\n",
      "../data/raw/digitize_screenshots\\SPX_15_LSC03_1month_20130826_trial_5_n2.csv\n",
      "../data/raw/digitize_screenshots\\202_31_trial_5_flow.csv\n",
      "../data/raw/digitize_screenshots\\202_31_trial_5_flow.csv\n",
      "../data/raw/digitize_screenshots\\213_26_trial_5_flow.csv\n",
      "../data/raw/digitize_screenshots\\213_27_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\213_27_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\213_27_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\213_27_trial_4_flow.csv\n",
      "../data/raw/digitize_screenshots\\213_27_trial_5_flow.csv\n",
      "../data/raw/digitize_screenshots\\213_27_trial_6_flow.csv\n",
      "../data/raw/digitize_screenshots\\215_25_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\215_31_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\215_31_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\215_31_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\215_31_trial_4_flow.csv\n",
      "../data/raw/digitize_screenshots\\215_31_trial_5_flow.csv\n",
      "../data/raw/digitize_screenshots\\215_31_trial_6_flow.csv\n",
      "../data/raw/digitize_screenshots\\216_26_trial_12_flow.csv\n",
      "../data/raw/digitize_screenshots\\256_27_trial_8_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC01_25_trial_7_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC07_21_trial_9_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC07_26_trial_11_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC07_26_trial_6_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC12.0.9_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC12.0.9_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC18_22_trial_5_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC34_24_1_trial_4_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC34_30_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_21_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_21_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_21_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_21_trial_4_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_21_trial_5_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_21_trial_6_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_22_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_22_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_22_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_22_trial_4_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_23_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_23_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_23_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_23_trial_4_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_23_trial_5_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_24_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_24_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_24_trial_4_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_24_trial_5_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC44_25_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC44_25_trial_4_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC44_25_trial_6_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC44_25_trial_7_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC47_24_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC49_21_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC49_21_trial_4_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC50_23_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC50_23_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC50_23_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC50_25_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC50_25_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC50_25_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC52_24_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC52_24_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC52_24_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_25_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_25_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_25_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_26_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_26_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_26_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_26_trial_4_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC54_27_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC56_25_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC56_25_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC56_25_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC56_25_trial_4_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC56_27_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC56_27_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC56_27_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC64_26_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC64_26_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC64_26_trial_4_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC68_23_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC68_23_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC68_23_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC68_24_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC68_24_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC68_24_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC68_24_trial_4_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC69_23_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC69_23_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC72_26_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC72_26_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSC72_26_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSH09_24_trial_9_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSH19_29_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSH22_29_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSH31_24_2_trial_5_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSH31_28_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSH33_24_1_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSH33_26_trial_10_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSH33_26_trial_6_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSH33_26_trial_8_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSH40_24_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSH40_24_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\LSH40_27_trial_3_flow.csv\n",
      "../data/raw/digitize_screenshots\\SPX_164_LSC08_3month_20130823_trial_2_flow.csv\n",
      "../data/raw/digitize_screenshots\\SPX_165_LSC20_6month_20140401_trial_1_flow.csv\n",
      "../data/raw/digitize_screenshots\\SPX_313_LSH24_6month_20150102_trial_11_flow.csv\n",
      "../data/raw/digitize_screenshots\\212_21_trial_6_volume.csv\n",
      "../data/raw/digitize_screenshots\\213_27_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\215_31_trial_6_volume.csv\n",
      "../data/raw/digitize_screenshots\\232.5_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\238_25_trial_12_volume.csv\n",
      "../data/raw/digitize_screenshots\\242_22_2_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\242_22_2_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\242_22_2_trial_4_volume.csv\n",
      "../data/raw/digitize_screenshots\\245_24_trial_4_volume.csv\n",
      "../data/raw/digitize_screenshots\\256.1_trial_6_volume.csv\n",
      "../data/raw/digitize_screenshots\\259_21_trial_10_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC10_24_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC16_28_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC28_29_trial_4_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC29_25_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC35_21_trial_6_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC38_21_trial_5_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_23_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_23_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_23_trial_4_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_23_trial_5_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC41_30_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC44_21_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC44_21_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC44_21_trial_5_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC44_21_trial_7_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC44_21_trial_8_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC44_25_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC44_25_trial_4_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC44_25_trial_6_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC45_24_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC46_21_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC46_21_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC47_24_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC48_25_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC48_25_trial_5_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC48_26_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC48_26_trial_5_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC48_26_trial_6_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC50_29_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC50_29_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC51_29_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC51_29_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_23_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_23_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_23_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_24_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_25_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_25_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_25_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_27_trial_4_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_28_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_28_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC53_28_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC54_22_trial_4_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC54_27_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC56_25_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC56_25_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC56_25_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC56_26_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC59_24_trial_7_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC59_28_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC59_28_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC59_29_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC59_29_trial_4_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC60_23_trial_4_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC60_25_trial_8_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC60_30_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC60_30_trial_4_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC64_26_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC64_26_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC64_26_trial_4_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC67_29_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC67_29_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC68_22_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC68_28_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC69_24_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC69_24_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC69_24_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC69_24_trial_4_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC72_21_trial_5_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC72_26_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC72_26_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC72_26_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC73_25_trial_4_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC74_28_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC75_22_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC75_22_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC75_26_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC75_26_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC75_30_trial_1_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC76_21_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC79_23_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC79_28_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC79_28_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSC79_29_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSH37_21_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSH40_23_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\LSH40_27_trial_3_volume.csv\n",
      "../data/raw/digitize_screenshots\\SPX_14_LSC10_3month_20130924_trial_5_volume.csv\n",
      "../data/raw/digitize_screenshots\\SPX_164_LSC08_3month_20130823_trial_2_volume.csv\n",
      "../data/raw/digitize_screenshots\\SPX_164_LSC08_3month_20130823_trial_6_volume.csv\n",
      "../data/raw/digitize_screenshots\\SPX_164_LSC08_3month_20130823_trial_7_volume.csv\n",
      "../data/raw/digitize_screenshots\\SPX_164_LSC08_3month_20130823_trial_8_volume.csv\n",
      "../data/raw/digitize_screenshots\\SPX_196_LSH02_3month_20131220_trial_7_volume.csv\n",
      "../data/raw/digitize_screenshots\\SPX_263_LSH12_1month_20140112_trial_8_volume.csv\n"
     ]
    }
   ],
   "source": [
    "# outlier values to manually check\n",
    "min_max ={\n",
    "    'o2': {'l_bound': -5, 'u_bound': 105}, \n",
    "    'co2': {'l_bound': -5, 'u_bound': 105}, \n",
    "    'n2': {'l_bound': -5, 'u_bound': 105}, \n",
    "    'flow':{'l_bound': -4000, 'u_bound': 4000}, \n",
    "    'volume':{'l_bound': -4000, 'u_bound': 4000}\n",
    "}\n",
    "\n",
    "for table_type in ['o2', 'co2', 'n2', 'flow', 'volume']:\n",
    "    tables_list = []\n",
    "    # get path of all txt files in the breath tables folder\n",
    "    for root, dirs, files in os.walk('../data/raw/digitize_screenshots'):\n",
    "        for file in files:\n",
    "            if file.endswith('_{}.csv'.format(table_type)):\n",
    "                tables_list.append(os.path.join(root, file))\n",
    "\n",
    "    for single_table in tables_list:\n",
    "        try:\n",
    "            table_temp = pd.read_csv(\n",
    "                single_table, header=None, delim_whitespace=True\n",
    "            )\n",
    "\n",
    "            if table_temp.iloc[:, 1].max() > min_max[table_type]['u_bound']:\n",
    "                print(single_table)\n",
    "            if table_temp.iloc[:, 1].min() < min_max[table_type]['l_bound']:\n",
    "                print(single_table)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digitized values with outliers (above) from `mbw_qc/mbw_qc/data/3-digitize_screenshot.py` were manually compared to the screenshots obtained from `mbw_qc/mbw_qc/data/1-spiroware_screenshot.py`. Only digitized screenshots that differed from the source screenshot were futher investigated. Most of the issues occured because the test was longer than normal or there was a higher than normal breath frequency. This resulted in an occlusion of the axis which could not be identired by the `mbw_qc/mbw_qc/data/3-digitize_screenshot.py` script. The relevant issues were placed in the `data/raw/digitize_screenshots_manual` folder and manually corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "screenshot_issues = [\n",
    "    '../data/raw/spiroware_screenshots/216.6_trial_4_o2.png',\n",
    "    '../data/raw/spiroware_screenshots/241_25_trial_3_o2.png',\n",
    "    '../data/raw/spiroware_screenshots/245_29_trial_4_o2.png',\n",
    "    '../data/raw/spiroware_screenshots/313.2_trial_1_o2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC02_22_trial_3_o2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC02_28_trial_5_o2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC46_25_trial_1_o2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC49_26_trial_1_o2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC59_29_trial_1_o2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC68_26_trial_3_o2.png',\n",
    "    '../data/raw/spiroware_screenshots/203_24_trial_6_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/205_24_trial_3_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/205_30_trial_2_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/206_21_trial_2_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/231_22_trial_6_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/242_25_trial_5_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/245_29_trial_4_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC02_22_trial_3_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC02_28_trial_5_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC03_30_trial_6_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC09_27_trial_1_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC12_29_trial_4_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC18_23_trial_2_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC18_23_trial_6_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_22_trial_2_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_22_trial_4_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_30_trial_3_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC43_28_trial_3_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC45_21_trial_2_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC45_21_trial_3_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC45_23_trial_1_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC49_26_trial_1_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC49_26_trial_2_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC49_27_trial_3_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC49_28_trial_1_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_30_trial_2_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC56_25_trial_4_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC59_29_trial_1_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC59_29_trial_3_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC59_29_trial_5_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC61_22_trial_4_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC61_31_trial_5_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC68_26_trial_3_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC77_21_trial_2_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC78_27_trial_2_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/SPX_15_LSC03_1month_20130826_trial_5_n2.png',\n",
    "    '../data/raw/spiroware_screenshots/202_31_trial_5_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/202_31_trial_5_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/213_26_trial_5_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/213_27_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/213_27_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/213_27_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/213_27_trial_4_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/213_27_trial_5_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/213_27_trial_6_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/215_25_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/215_31_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/215_31_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/215_31_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/215_31_trial_4_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/215_31_trial_5_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/215_31_trial_6_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/216_26_trial_12_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/256_27_trial_8_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC01_25_trial_7_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC07_21_trial_9_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC07_26_trial_11_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC07_26_trial_6_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC12.0.9_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC12.0.9_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC18_22_trial_5_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC34_24_1_trial_4_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC34_30_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_21_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_21_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_21_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_21_trial_4_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_21_trial_5_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_21_trial_6_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_22_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_22_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_22_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_22_trial_4_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_23_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_23_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_23_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_23_trial_4_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_23_trial_5_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_24_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_24_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_24_trial_4_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_24_trial_5_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC44_25_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC44_25_trial_4_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC44_25_trial_6_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC44_25_trial_7_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC47_24_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC49_21_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC49_21_trial_4_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC50_23_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC50_23_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC50_23_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC50_25_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC50_25_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC50_25_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC52_24_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC52_24_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC52_24_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_25_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_25_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_25_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_26_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_26_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_26_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_26_trial_4_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC54_27_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC56_25_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC56_25_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC56_25_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC56_25_trial_4_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC56_27_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC56_27_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC56_27_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC64_26_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC64_26_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC64_26_trial_4_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC68_23_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC68_23_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC68_23_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC68_24_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC68_24_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC68_24_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC68_24_trial_4_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC69_23_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC69_23_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC72_26_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC72_26_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC72_26_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSH09_24_trial_9_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSH19_29_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSH22_29_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSH31_24_2_trial_5_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSH31_28_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSH33_24_1_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSH33_26_trial_10_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSH33_26_trial_6_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSH33_26_trial_8_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSH40_24_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSH40_24_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/LSH40_27_trial_3_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/SPX_164_LSC08_3month_20130823_trial_2_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/SPX_165_LSC20_6month_20140401_trial_1_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/SPX_313_LSH24_6month_20150102_trial_11_flow.png',\n",
    "    '../data/raw/spiroware_screenshots/212_21_trial_6_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/213_27_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/215_31_trial_6_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/232.5_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/238_25_trial_12_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/242_22_2_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/242_22_2_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/242_22_2_trial_4_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/245_24_trial_4_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/256.1_trial_6_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/259_21_trial_10_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC10_24_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC16_28_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC28_29_trial_4_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC29_25_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC35_21_trial_6_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC38_21_trial_5_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_23_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_23_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_23_trial_4_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_23_trial_5_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC41_30_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC44_21_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC44_21_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC44_21_trial_5_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC44_21_trial_7_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC44_21_trial_8_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC44_25_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC44_25_trial_4_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC44_25_trial_6_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC45_24_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC46_21_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC46_21_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC47_24_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC48_25_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC48_25_trial_5_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC48_26_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC48_26_trial_5_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC48_26_trial_6_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC50_29_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC50_29_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC51_29_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC51_29_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_23_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_23_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_23_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_24_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_25_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_25_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_25_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_27_trial_4_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_28_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_28_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC53_28_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC54_22_trial_4_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC54_27_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC56_25_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC56_25_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC56_25_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC56_26_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC59_24_trial_7_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC59_28_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC59_28_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC59_29_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC59_29_trial_4_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC60_23_trial_4_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC60_25_trial_8_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC60_30_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC60_30_trial_4_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC64_26_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC64_26_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC64_26_trial_4_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC67_29_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC67_29_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC68_22_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC68_28_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC69_24_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC69_24_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC69_24_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC69_24_trial_4_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC72_21_trial_5_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC72_26_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC72_26_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC72_26_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC73_25_trial_4_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC74_28_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC75_22_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC75_22_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC75_26_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC75_26_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC75_30_trial_1_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC76_21_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC79_23_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC79_28_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC79_28_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSC79_29_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSH37_21_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSH40_23_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/LSH40_27_trial_3_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/SPX_14_LSC10_3month_20130924_trial_5_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/SPX_164_LSC08_3month_20130823_trial_2_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/SPX_164_LSC08_3month_20130823_trial_6_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/SPX_164_LSC08_3month_20130823_trial_7_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/SPX_164_LSC08_3month_20130823_trial_8_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/SPX_196_LSH02_3month_20131220_trial_7_volume.png',\n",
    "    '../data/raw/spiroware_screenshots/SPX_263_LSH12_1month_20140112_trial_8_volume.png',\n",
    "\n",
    "]\n",
    "\n",
    "for screenshot_issue in screenshot_issues:\n",
    "    shutil.copy(screenshot_issue, '../data/raw/spiroware_screenshots_manual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical output looked like Figure 2 after successfully digitizing.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/digitize_example.png\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  Figure 2. Typical digitization (right column) of the Spiroware screenshot (left column). From top to bottom, CO<sub>2</sub>, O<sub>2</sub>, N<sub>2</sub>, flow, volume.\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.ii: Organize Files<a name=\"section-2ii-data\"></a>\n",
    "After all the raw data was obtained, it was necessary to link the associated feature data with the correct trial data from REDCap. The REDCap data contains the grade and outcome (i.e. labels used for training). The end goal of this section was to create a dataframe where each row represents an individual trial and the associated path of the feature file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2.ii.a: Clean and combine REDCap<a name=\"section-2iia-clean-rc\"></a>\n",
    "Since the data came from more than one study, it was necessary to combine the REDCap data into a single data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LONGITUDINAL_REDCAP_PATH = (\n",
    "    '../data/external/Primary Training Data Set/PS LONGITUDINAL DATA SET/',\n",
    "    'RedCap Quality Control Export/longitudinal_redcap_qc-20AUG2021.csv'\n",
    ")\n",
    "TRACK_REDCAP_PATH = (\n",
    "    '../data/external/Primary Training Data Set/TRACK DATA SET/',\n",
    "    'Track RedCap QC Export/Track RedCap QC Export/track_redcap_qc-16JUL2021.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LONGITUDINAL data and manipulate\n",
    "longitudinal_redcap_qc = pd.read_csv(LONGITUDINAL_REDCAP_PATH)\n",
    "longitudinal_redcap_qc = longitudinal_redcap_qc[[\n",
    "    'studyno_rev', 'test_occasion', 'spx_filename', 'trial', \n",
    "    'trial_accepted', 'trial_accepted_label', 'qc_grade', 'qc_grade_label'\n",
    "]]\n",
    "longitudinal_redcap_qc.rename(columns={'studyno_rev': 'id'}, inplace=True)\n",
    "longitudinal_redcap_qc.loc[\n",
    "    longitudinal_redcap_qc['qc_grade'] == 7, 'qc_grade_label'\n",
    "] = 'N/A'\n",
    "\n",
    "# Load the TRACK data and manipulate\n",
    "track_redcap_qc = pd.read_csv(TRACK_REDCAP_PATH)\n",
    "track_redcap_qc = track_redcap_qc[[\n",
    "    'id', 'trial', 'trial_accepted', 'trial_accepted_label', 'qc_grade', \n",
    "    'qc_grade_label'\n",
    "]]\n",
    "\n",
    "# TRACK file is missing 'spx_filename' and 'test_occasion'\n",
    "# 'id' is equivalent to 'spx_filename'; incorporates 'test_occasion' info\n",
    "track_redcap_qc['spx_filename'] = track_redcap_qc['id']\n",
    "\n",
    "# only a few TRACK subjects have more than one test occasion; will have an \n",
    "# appended value (i.e. '_1' or '_2') to indicate test occasion\n",
    "track_redcap_qc['id'] = (\n",
    "    track_redcap_qc['spx_filename'].str.replace('_[12]$', '', regex=True)\n",
    ")\n",
    "track_redcap_qc['test_occasion'] = (\n",
    "    track_redcap_qc['spx_filename'].str.replace('^[^_]+_[^_]+_*', '', regex=True)\n",
    ")\n",
    "track_redcap_qc.loc[\n",
    "    track_redcap_qc['test_occasion'] == '', 'test_occasion'\n",
    "] = '1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some TRACK instances were 'Rejected' but were not associated with a grade. The original data was reviewed to confirm the 'Rejected' instances were 'N/A' grades.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_TRACK_REDCAP_PATH = '../data/external/Raw REDCap QC/TRACK_QC.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_track_redcap_qc = pd.read_csv(ORIG_TRACK_REDCAP_PATH)\n",
    "\n",
    "aborted_col_name = []\n",
    "for i in range(1, 20+1):\n",
    "    old_col = 'not_accepted_why{}___7'.format(str(i))\n",
    "    new_col = 'not_accepted_why___7{}'.format(str(i))\n",
    "    aborted_col_name.append(new_col)\n",
    "    orig_track_redcap_qc.rename(columns={old_col: new_col}, inplace=True)\n",
    "\n",
    "aborted_col_name.append('visit_id')\n",
    "track_aborted = pd.wide_to_long(\n",
    "    orig_track_redcap_qc[\n",
    "        orig_track_redcap_qc.columns.intersection(aborted_col_name)\n",
    "    ], \n",
    "    ['not_accepted_why___7'], i='visit_id', j='trial'\n",
    ")\n",
    "\n",
    "track_aborted = track_aborted.reset_index(level=['visit_id', 'trial'])\n",
    "track_aborted['id_trial'] = (\n",
    "    'track_id_{}_trial_{}'.format(\n",
    "        track_aborted['visit_id'], track_aborted['trial'].astype('str')\n",
    "    )\n",
    ")\n",
    "track_aborted_id_trial = track_aborted.loc[\n",
    "    track_aborted['not_accepted_why___7'] == 1, 'id_trial'\n",
    "].tolist()\n",
    "\n",
    "# combine the original and the summary data together\n",
    "track_redcap_qc['id_trial'] = (\n",
    "    'track_id_{}_trial_{}'.format(\n",
    "        track_redcap_qc['id'], track_redcap_qc['trial'].astype('str')\n",
    "    )\n",
    ")\n",
    "track_redcap_qc.loc[\n",
    "   track_redcap_qc['id_trial'].isin(track_aborted_id_trial)\n",
    "   & track_redcap_qc['qc_grade_label'].isna(),\n",
    "   'qc_grade_label'\n",
    "] = 'N/A'\n",
    "track_redcap_qc.drop(['id_trial'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the TRACK and LONGITUDINAL REDCap data are in the same format. The data sets were then combined and modified. \n",
    "\n",
    "The grades 'A/B' and 'C' should correspond to an 'Accepted' decision while the grades 'D', 'E', 'F', and 'N/A' should correspond to a 'Rejected' trial. The grades and trial decision was checked to confirm consistency between trials and tests (a test contains multiple trials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted    4394\n",
      "Name: trial_accepted_label, dtype: int64\n",
      "Accepted    1767\n",
      "Name: trial_accepted_label, dtype: int64\n",
      "Rejected    1056\n",
      "Name: trial_accepted_label, dtype: int64\n",
      "Rejected    101\n",
      "Name: trial_accepted_label, dtype: int64\n",
      "Rejected    2913\n",
      "Name: trial_accepted_label, dtype: int64\n",
      "Rejected    3071\n",
      "Name: trial_accepted_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "redcap_qc = pd.concat([track_redcap_qc, longitudinal_redcap_qc])\n",
    "\n",
    "# drop columns with missing information\n",
    "redcap_qc = redcap_qc.loc[\n",
    "    redcap_qc['trial_accepted_label'].notna()\n",
    "    & redcap_qc['qc_grade_label'].notna()\n",
    "]\n",
    "\n",
    "# remove trials deemed inappropriate by site\n",
    "redcap_qc = redcap_qc.loc[\n",
    "    redcap_qc['trial_accepted_label'] != 'Appropriately excluded by site'\n",
    "]\n",
    "\n",
    "# grades A and B are combined for some trials and not others; \n",
    "# make it consistent throughout\n",
    "redcap_qc.loc[\n",
    "    (\n",
    "        (redcap_qc['qc_grade_label'] == 'A') \n",
    "        | (redcap_qc['qc_grade_label'] == 'B')\n",
    "    ), 'qc_grade_label'\n",
    "] = 'A/B'\n",
    "\n",
    "# since \"Rejected\" trial decision was modified (i.e. additional \"N/A\"), change \n",
    "# the trial decision for all associated grades\n",
    "redcap_qc.loc[\n",
    "    (\n",
    "        (redcap_qc['qc_grade_label'] == 'D')\n",
    "        | (redcap_qc['qc_grade_label'] == 'E')\n",
    "        | (redcap_qc['qc_grade_label'] == 'F')\n",
    "        | (redcap_qc['qc_grade_label'] == 'N/A')\n",
    "    ),\n",
    "    'trial_accepted_label'\n",
    "] = 'Rejected'\n",
    "\n",
    "# check for consistency between trial and test\n",
    "for qc_grade_label in ['A/B', 'C', 'D', 'E', 'F', 'N/A']:\n",
    "    print(redcap_qc.loc[\n",
    "        redcap_qc['qc_grade_label'] == qc_grade_label, 'trial_accepted_label'\n",
    "    ].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2.ii.b: Link raw data<a name=\"section-2iib-link-raw\"></a>\n",
    "There is now a data set with all the labels (i.e. grades and trial outcomes). The labels must now be linked with the raw feature files.\n",
    "\n",
    "To aid in linking the features to the labels, a function was created to get all the path names in a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_file_paths(folder_path, file_ext = '.txt'):\n",
    "    \"\"\"Get path of all files in all folder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        Path to folder of interest\n",
    "    file_ext : str, optional\n",
    "        File extension of interest; will only return files ending in file_ext, \n",
    "        by default '.txt'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        Contains the file paths in folder_path of all files ending in file_ext\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(file_ext):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_BREATH_TABLES_PATH = (\n",
    "    '../data/external/Primary Training Data Set/TRACK DATA SET/',\n",
    "    'Track Breath Tables TXT/'\n",
    ")\n",
    "TRACK_TBFVL_TABLES_PATH = (\n",
    "    '../data/external/Primary Training Data Set/TRACK DATA SET/',\n",
    "    'Track TBFVL Tables/'\n",
    ")\n",
    "LONG_BREATH_TABLES_PATH = (\n",
    "    '../data/external/Primary Training Data Set/PS LONGITUDINAL DATA SET/',\n",
    "    'breathtables/breath'\n",
    ")\n",
    "LONG_TBFVL_TABLES_PATH = (\n",
    "    '../data/external/Primary Training Data Set/PS LONGITUDINAL DATA SET/',\n",
    "    'tbfvltables/tbfvl'\n",
    ")\n",
    "SCREENSHOTS_PATH = '../data/raw/digitize_screenshots'\n",
    "SPX_PATH = \"../data/raw/spx_exports/\"\n",
    "SCREENSHOTS_PATH_MANUAL = '../data/raw/digitize_screenshots_manual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "breath_tables = (\n",
    "    get_file_paths(TRACK_BREATH_TABLES_PATH) \n",
    "    + get_file_paths(LONG_BREATH_TABLES_PATH)\n",
    ")\n",
    "tbfvl_tables = (\n",
    "    get_file_paths(TRACK_TBFVL_TABLES_PATH)\n",
    "    + get_file_paths(LONG_TBFVL_TABLES_PATH)\n",
    ")\n",
    "spx = get_file_paths(SPX_PATH, file_ext='.csv')\n",
    "\n",
    "screenshots_path = get_file_paths(SCREENSHOTS_PATH, file_ext='.csv')\n",
    "screenshots_path_manual = get_file_paths(\n",
    "    SCREENSHOTS_PATH_MANUAL, file_ext='.csv'\n",
    ")\n",
    "\n",
    "# if there is a file that was manually manipulated (Section 2.i.c)\n",
    "# (i.e. in 'digitize_screenshots_manual'), use that version\n",
    "screenshots_path = [\n",
    "    screenshot.replace('digitize_screenshots', 'digitize_screenshots_manual') \n",
    "    if screenshot.replace(\n",
    "        'digitize_screenshots', 'digitize_screenshots_manual'\n",
    "    ) in screenshots_path_manual \n",
    "    else screenshot\n",
    "    for screenshot in screenshots_path\n",
    "]\n",
    "\n",
    "co2s = [\n",
    "    screenshot \n",
    "    for screenshot in screenshots_path if re.search('_co2', screenshot)\n",
    "]\n",
    "n2s = [\n",
    "    screenshot \n",
    "    for screenshot in screenshots_path if re.search('_n2', screenshot)\n",
    "]\n",
    "flows = [\n",
    "    screenshot \n",
    "    for screenshot in screenshots_path if re.search('_flow', screenshot)\n",
    "]\n",
    "o2s = [\n",
    "    screenshot \n",
    "    for screenshot in screenshots_path if re.search('_o2', screenshot)\n",
    "]\n",
    "volumes = [\n",
    "    screenshot \n",
    "    for screenshot in screenshots_path if re.search('_volume', screenshot)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the file paths are collected in the associated objects (i.e. `breath_tables`, `tbfvl_tables`, `spx`, `co2s`, `n2s`, `flows`, `o2s`, `volumes`). A subject has more than one trial so it is necessary to assign the correct file path to the correct subject id-trial combination - the function `get_table_path` was created to map the correct file path to the correct subject id-trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_path(id_trials, table_list, is_screenshot=False):\n",
    "    \"\"\"Order file paths based on id-trial combinations\n",
    "\n",
    "    Will order the associated file paths from table_list based on id-trial \n",
    "    combinations in id_trial_tuples\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id_trial_tuples : zip\n",
    "        Contains id and trial used to order table_list\n",
    "    table_list : list of str\n",
    "        Contains all file paths of a particular table type\n",
    "    is_screenshot: bool\n",
    "        Indicates if table_list is list of screenshots, by default 'False'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        Contains the file paths ordered according to id-trial\n",
    "    \"\"\"\n",
    "    final_list = []\n",
    "    \n",
    "    for id_trial in id_trials:\n",
    "        id, trial = id_trial\n",
    "        \n",
    "        add_path = None\n",
    "        for path in table_list:\n",
    "            if is_screenshot:\n",
    "                if(\n",
    "                    bool(re.search(('{}_'.format(id)), path)) \n",
    "                    & bool(re.search(('trial_{}_'.format(str(trial))), path))\n",
    "                ):\n",
    "                    add_path = path\n",
    "                    break\n",
    "            else:\n",
    "                if(\n",
    "                    bool(re.search(('-{}-'.format(id)), path)) \n",
    "                    & bool(re.search(('Trial-{}-'.format(str(trial))), path))\n",
    "                ):\n",
    "                    add_path = path\n",
    "                    break\n",
    "\n",
    "        final_list.append(add_path)   \n",
    "         \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a new column to the REDCap dataframe that contains the file paths in the corresponding order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "redcap_qc['spx_export_path'] = get_table_path(\n",
    "    zip(redcap_qc['spx_filename'], redcap_qc['trial']), spx, True\n",
    ")\n",
    "redcap_qc['breath_path'] = get_table_path(\n",
    "    zip(redcap_qc['spx_filename'], redcap_qc['trial']), breath_tables\n",
    ")\n",
    "redcap_qc['tbfvl_path'] = get_table_path(\n",
    "    zip(redcap_qc['spx_filename'], redcap_qc['trial']), tbfvl_tables\n",
    ")\n",
    "redcap_qc['o2_path'] = get_table_path(\n",
    "    zip(redcap_qc['spx_filename'], redcap_qc['trial']), o2s, True\n",
    ")\n",
    "redcap_qc['n2_path'] = get_table_path(\n",
    "    zip(redcap_qc['spx_filename'], redcap_qc['trial']), n2s, True\n",
    ")\n",
    "redcap_qc['flow_path'] = get_table_path(\n",
    "    zip(redcap_qc['spx_filename'], redcap_qc['trial']), flows, True\n",
    ")\n",
    "redcap_qc['co2_path'] = get_table_path(\n",
    "    zip(redcap_qc['spx_filename'], redcap_qc['trial']), co2s, True\n",
    ")\n",
    "redcap_qc['volume_path'] = get_table_path(\n",
    "    zip(redcap_qc['spx_filename'], redcap_qc['trial']), volumes, True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set can be checked to make sure the assignment was done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n",
      "[None]\n",
      "[None]\n",
      "[None]\n",
      "[None]\n",
      "[None]\n",
      "[None]\n",
      "[None]\n"
     ]
    }
   ],
   "source": [
    "# check if file(s) are assigned to more than one trial\n",
    "for col_name in [\n",
    "    'spx_export_path', 'breath_path', 'tbfvl_path', 'o2_path', \n",
    "    'n2_path', 'flow_path', 'co2_path', 'volume_path'\n",
    "]:\n",
    "    print([\n",
    "        item \n",
    "        for item, count in collections.Counter(redcap_qc[col_name].tolist()).items() \n",
    "        if count > 1\n",
    "    ])\n",
    "\n",
    "# these were manually reviewed\n",
    "# usually an issue with number of trials in spx not matching number of breath/tbfvl tables\n",
    "# want to remove all trials associated with the spx file due to potential grade-trial misalignment\n",
    "spx_filename_remove = redcap_qc.loc[redcap_qc[\n",
    "    [\n",
    "        'spx_export_path', 'breath_path', 'tbfvl_path', 'o2_path', \n",
    "        'n2_path', 'flow_path', 'co2_path', 'volume_path'\n",
    "    ]\n",
    "].isna().any(axis=1), 'spx_filename'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A/B    4394\n",
       "N/A    3071\n",
       "F      2913\n",
       "C      1767\n",
       "D      1056\n",
       "E       101\n",
       "Name: qc_grade_label, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "redcap_qc['qc_grade_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.ii.c: Split data set<a name=\"section-2iic-split-data\"></a>\n",
    "The data set now contains the labels and the paths to the raw files which contain the features. The next step was to split the data. Since subjects can have more than one trial, the data is not independent and identically distributed; therefore, it was necessary to split the data by subjects instead of randomly splitting by trial. Randomly splitting by trial could lead to [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)#Training_example_leakage), an instance where the model is tested on a data set which is not representative the data used in subsequent predications. Data was split into a train (~75%), validation (~12.5%), and test (~12.5%) data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A/B    3282\n",
      "N/A    2182\n",
      "F      2165\n",
      "C      1287\n",
      "D       812\n",
      "E        62\n",
      "Name: qc_grade_label, dtype: int64\n",
      "A/B    545\n",
      "N/A    515\n",
      "F      369\n",
      "C      272\n",
      "D      125\n",
      "E       17\n",
      "Name: qc_grade_label, dtype: int64\n",
      "A/B    567\n",
      "F      379\n",
      "N/A    374\n",
      "C      208\n",
      "D      119\n",
      "E       22\n",
      "Name: qc_grade_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_prop = 0.75\n",
    "validate_prop = 0.125\n",
    "# test proportion implicitly is 1 - train_prop - validate_prop\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "# we want to split by subject to avoid data leakage\n",
    "unique_ids = redcap_qc['id'].unique()\n",
    "np.random.shuffle(unique_ids)\n",
    "\n",
    "train_temp, validate_temp, test_temp = np.split(\n",
    "        unique_ids,\n",
    "        [\n",
    "            int(train_prop*len(unique_ids)), \n",
    "            int((train_prop + validate_prop)*len(unique_ids))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# create new column based on lists\n",
    "redcap_qc['split_group'] = np.where(\n",
    "    redcap_qc['id'].isin(train_temp), 'train', np.where(\n",
    "        redcap_qc['id'].isin(test_temp), 'test', \n",
    "        'validate'\n",
    "    )\n",
    ")\n",
    "\n",
    "# data checks\n",
    "redcap_qc['split_group'].value_counts()\n",
    "for group_type in ['train', 'test', 'validate']:\n",
    "    print(redcap_qc.loc[\n",
    "        redcap_qc['split_group'] == group_type,'qc_grade_label'\n",
    "    ].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have achieved our goal for this Section 2.ii. There is a dataframe were each row represents an individual trial and contains label information as well as the associated path of the feature files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy of the dataframe\n",
    "# probably should have done another data check before saving\n",
    "redcap_qc[\n",
    "    ~redcap_qc['spx_filename'].isin(spx_filename_remove)\n",
    "].to_csv('../data/intermediary/main_id_associated_files.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.iii: Preprocess Data<a name=\"section-2i-data\"></a>\n",
    "Although we now have a dataframe containing the labels and the location of the feature files, the feature files still need to be preprocessed before being used in the model. By the end of this section, information that will used in the padding, interpolation, and standardization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = '../data/raw/'\n",
    "INTERMEDIARY_PATH = '../data/intermediary/'\n",
    "PROCESSED_PATH = '../data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "redcap_qc = pd.read_csv(os.path.join(\n",
    "    INTERMEDIARY_PATH, 'main_id_associated_files.csv'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When processing sequence data, it is necessary to ensure that all the data have the standard length or size. Adding additional rows so the sequence data is a uniform size is a process known as 'padding'. In order to facilitate this process, it was necessary to find the maximum number of steps in each file. For the breath and TBFVL table, a step corresponds to a single breath. For the SPX data, it was not necessary to find the number of steps since they are uniform in the data set. The maximum number of steps will be used to pad the sequences to a uniform size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breath\n",
      "170\n",
      "tbfvl\n",
      "185\n",
      "o2\n",
      "572.002\n",
      "n2\n",
      "572.002\n",
      "co2\n",
      "574.701\n",
      "flow\n",
      "572.002\n",
      "volume\n",
      "572.002\n"
     ]
    }
   ],
   "source": [
    "# find the maximum number of steps (rows) in the files\n",
    "# go through all breath tables and find the maximum number of rows;\n",
    "# this information will be used in padding the files\n",
    "for table_type in ['breath', 'tbfvl']:\n",
    "    max_steps = 0\n",
    "    for single_table in redcap_qc[table_type+'_path'].to_list():\n",
    "        table_temp = pd.read_csv(single_table, sep='\\t')\n",
    "\n",
    "        if table_temp.shape[0] > max_steps:\n",
    "            max_steps = table_temp.shape[0]\n",
    "    print(table_type)\n",
    "    print(str(max_steps))\n",
    "\n",
    "# find the maximum number of steps (rows) in the files\n",
    "# go through all digitized signals and find the maximum number of rows;\n",
    "# this information will be used in padding the files\n",
    "for table_type in ['o2', 'n2', 'co2', 'flow', 'volume']:\n",
    "    max_steps = 0\n",
    "    for single_table in redcap_qc[table_type+'_path']:\n",
    "        try:\n",
    "            table_temp  = pd.read_csv(\n",
    "                single_table, header=None, delim_whitespace=True\n",
    "            )\n",
    "            if table_temp.iloc[:, 0].max() > max_steps:\n",
    "                max_steps = table_temp.iloc[:, 0].max()\n",
    "        # ignore files with no values\n",
    "        except pd.errors.EmptyDataError:\n",
    "            pass        \n",
    "    print(table_type)\n",
    "    print(str(max_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The digitization of the raw screenshot data results in a non-uniform time steps. It is necessary to interpolate the screenshot so that there is uniform magnitude between time steps.\n",
    "\n",
    "Additionally, the feature data will be standardized which improves the performance of the model. Standardization is accomplished by applying the following function to each feature.\n",
    "$$z=\\frac{x-\\mu}{\\sigma}$$\n",
    "where the $\\mu$ represents the mean and $\\sigma$ represents the standard deviation. Therefore it is necessary to get both the mean and standard deviation for each feature.\n",
    "\n",
    "The following two functions outline interpolation and obtaining the mean and standard deviation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_screenshot(raw_ss, round_dig = 1):\n",
    "    \"\"\"Interpolate the screenshot values\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_ss : pandas.dataframe\n",
    "        Screenshot values to be interpolated\n",
    "    round_dig : int, optional\n",
    "        Number of digits to round the time, by default 1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.dataframe\n",
    "        Modified raw_ss dataframe with interpolated values\n",
    "    \"\"\"\n",
    "    raw_ss = raw_ss.rename(columns={0: 'time', 1: 'val'})\n",
    "    raw_ss['time'] = raw_ss['time'].round(round_dig)\n",
    "    raw_ss = raw_ss.drop_duplicates(subset='time', keep='first')\n",
    "\n",
    "    time_inc = 10**-(round_dig)\n",
    "    time_df = pd.DataFrame(\n",
    "        [\n",
    "            round(i, round_dig) \n",
    "            for i in np.arange(\n",
    "                time_inc, raw_ss['time'].max() + time_inc, time_inc\n",
    "            )\n",
    "        ], columns =['time']\n",
    "    )\n",
    "\n",
    "    inter_ss = (\n",
    "        raw_ss\n",
    "        .merge(time_df, on='time', how='outer')\n",
    "        .sort_values(by=['time'])\n",
    "        .interpolate(axis='rows', limit_direction='both')\n",
    "    )\n",
    "\n",
    "    # inconsistency with capturing 0, so remove\n",
    "    inter_ss = inter_ss.drop(inter_ss[inter_ss['time'] == 0].index)\n",
    "\n",
    "    return inter_ss\n",
    "\n",
    "\n",
    "def get_mean_sd(raw_df, ignore_cols=[]):\n",
    "    \"\"\"Get mean and standard deviation of columns in a dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_df : pandas.dataframe\n",
    "        Dataframe that we want to get the mean and standard deviations for\n",
    "    ignore_cols : list, optional\n",
    "        Columns to ignore, by default []\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.dataframe\n",
    "        Dataframe containing the mean and standard deviation for all columns of\n",
    "        raw_df\n",
    "    \"\"\"\n",
    "    raw_df = raw_df.drop(ignore_cols, axis=1)\n",
    "    raw_df.replace(\"\", np.nan, inplace=True)\n",
    "    raw_df_mean = raw_df.mean(axis=0)\n",
    "    raw_df_std = raw_df.std(axis=0)\n",
    "\n",
    "    processed_df = pd.concat(\n",
    "        [raw_df_mean, raw_df_std], axis=1\n",
    "    ).reset_index()\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `get_mean_sd()` function successfully, we need to use it on a data set. It is best practice to use training data instead of the entire data set so the test data is wholly unseen. Therefore, we need to combine all the data of the training data set and apply the `get_mean_sd()` function. The combined data and the dataframe with the mean and standard deviation is saved during the process. The process is repeated for each data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "breath_dfs = []\n",
    "# combine all data from the breath tables   \n",
    "for breath_path in redcap_qc.loc[\n",
    "        # only training data\n",
    "        redcap_qc['split_group'] == 'train', 'breath_path'\n",
    "    ].to_list():\n",
    "    breath_table = pd.read_csv(breath_path, sep='\\t')\n",
    "    try:\n",
    "        breath_table = breath_table.drop(\n",
    "            columns = [\n",
    "                'Excluded for Sacin/Scond calculation', 'VdCO2 Langley [ml]'\n",
    "            ]\n",
    "        )\n",
    "        breath_table = breath_table.rename(\n",
    "            columns={'VdCO2 Fowler [ml]': 'VdCO2 [ml]'}\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "    breath_dfs.append(breath_table)\n",
    "breath_df = pd.concat(breath_dfs, ignore_index=True)\n",
    "\n",
    "# save the dataframe containing all the training data\n",
    "breath_df.to_csv(\n",
    "    os.path.join(INTERMEDIARY_PATH, 'combine_data_type', 'breath_raw.csv'),\n",
    "    index = False\n",
    ")\n",
    "\n",
    "# replace inf value with min value of column\n",
    "for col_min in ['VdCO2 [ml]']:\n",
    "    col_min_val = breath_df.loc[breath_df[col_min] != np.NINF, col_min].min()\n",
    "    breath_df[col_min].replace(np.NINF, col_min_val, inplace=True)\n",
    "\n",
    "# get and save the mean and SD of the breath tables\n",
    "breath_mean_sd_df = get_mean_sd(breath_df)\n",
    "breath_mean_sd_df.to_csv(\n",
    "    os.path.join(INTERMEDIARY_PATH, 'combine_data_type', 'breath_mean_sd.csv')\n",
    ")\n",
    "\n",
    "# repeat the process for tbfvl tables\n",
    "tbfvl_dfs = []\n",
    "for tbfvl_path in redcap_qc.loc[\n",
    "        redcap_qc['split_group'] == 'train', 'tbfvl_path'\n",
    "    ].to_list():\n",
    "    tbfvl_table = pd.read_csv(tbfvl_path, sep='\\t')\n",
    "    tbfvl_dfs.append(tbfvl_table)\n",
    "tbfvl_df = pd.concat(tbfvl_dfs, ignore_index=True)\n",
    "\n",
    "tbfvl_df.to_csv(\n",
    "    os.path.join(INTERMEDIARY_PATH, 'combine_data_type', 'tbfvl_raw.csv'), \n",
    "    index = False\n",
    ")\n",
    "\n",
    "tbfvl_mean_sd_df = get_mean_sd(tbfvl_df, ['Phase', 'Timestamp (UTC)'])\n",
    "tbfvl_mean_sd_df.to_csv(\n",
    "    os.path.join(INTERMEDIARY_PATH, 'combine_data_type', 'tbfvl_mean_sd.csv')\n",
    ")\n",
    "\n",
    "# repeat the process for spx\n",
    "spx_dfs = []\n",
    "for spx_path in redcap_qc.loc[\n",
    "        redcap_qc['split_group'] == 'train', 'spx_export_path'\n",
    "    ].to_list():\n",
    "    spx = pd.read_csv(spx_path)\n",
    "    spx_dfs.append(spx)\n",
    "spx_df = pd.concat(spx_dfs, ignore_index=True)\n",
    "\n",
    "# replace inf value with max value of column\n",
    "for col_max in ['W slower', 'N2Cet norm @ TO6 [%]']:\n",
    "    col_max_val = spx_df.loc[spx_df[col_max] != np.inf, col_max].max()\n",
    "    spx_df[col_max].replace(np.inf, col_max_val, inplace=True)\n",
    "\n",
    "# replace inf value with min value of column\n",
    "for col_min in ['Vd CO2 mean [ml]']:\n",
    "    col_min_val = spx_df.loc[spx_df[col_min] != np.NINF, col_min].min()\n",
    "    spx_df[col_min].replace(np.NINF, col_min_val, inplace=True)\n",
    "\n",
    "# convert date of birth to unix time\n",
    "spx_df['Date of birth'] = (\n",
    "    pd.to_datetime(spx_df['Date of birth'], format='%d.%m.%Y')\n",
    "    - pd.Timestamp('1970-01-01')\n",
    ") // pd.Timedelta('1s')\n",
    "\n",
    "spx_df.to_csv(\n",
    "    os.path.join(INTERMEDIARY_PATH, 'combine_data_type', 'spx_raw.csv'),\n",
    "    index = False\n",
    ")\n",
    "\n",
    "spx_mean_sd_df = get_mean_sd(\n",
    "    spx_df.drop(columns=[\n",
    "        'Patient-ID', 'Lastname', 'Firstname', 'Gender', \n",
    "        'Ethnicity', 'Smoker', 'Asthma', 'Notes', 'Test Date (UTC)',\n",
    "        'Timestamp (UTC)', 'Comment', 'FRC @ TO6 [l]', 'Scond*VT [l]',\n",
    "        'Sacin*VT [l]', 'Pacin*VT [l]', 'Scond', 'Sacin','Pacin',\n",
    "    ])\n",
    ")\n",
    "\n",
    "spx_mean_sd_df.to_csv(\n",
    "    os.path.join(INTERMEDIARY_PATH, 'combine_data_type', 'spx_mean_sd.csv')\n",
    ")\n",
    "\n",
    "# repeat the process for raw MBW signals\n",
    "ss_dict = {}\n",
    "for ss_type in ['o2', 'n2', 'co2', 'flow', 'volume']:\n",
    "    ss_dfs = []\n",
    "    for ss_path in redcap_qc.loc[\n",
    "        redcap_qc['split_group'] == 'train', ss_type +'_path'\n",
    "    ].to_list():\n",
    "        try:\n",
    "            raw_ss = pd.read_csv(ss_path, header=None, delim_whitespace=True)\n",
    "            ss_dfs.append(interpolate_screenshot(raw_ss))\n",
    "        # ignore files with no values; no use for determining mean/SD\n",
    "        except pd.errors.EmptyDataError:\n",
    "            pass\n",
    "    ss_df = pd.concat(ss_dfs, ignore_index=True)\n",
    "    ss_df.to_csv(\n",
    "        os.path.join(\n",
    "            INTERMEDIARY_PATH, 'combine_data_type',\n",
    "            '{}_raw.csv'.format(ss_type)\n",
    "        ), \n",
    "        index = False\n",
    "    )\n",
    "\n",
    "    mean_sd_df = get_mean_sd(ss_df)\n",
    "    mean_sd_df.to_csv(\n",
    "        os.path.join(\n",
    "            INTERMEDIARY_PATH, 'combine_data_type',\n",
    "            '{}_mean_sd.csv'.format(ss_type)\n",
    "        )\n",
    "    )\n",
    "    ss_dict[ss_type] = mean_sd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining the mean and standard deviation, they can be printed out in the form of a dictionary. The mean and standard deviation will be incorporated into a function used for standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Breath #': {'mean': 24.945467123196384, 'sd': 18.281016819661676},\n",
      "'N2 Cet [%]': {'mean': 12.144732241076664, 'sd': 15.141453997518099},\n",
      "'TO': {'mean': 5.375122485703167, 'sd': 3.4907699447313254},\n",
      "'FRC [l]': {'mean': 1.0826127705424118, 'sd': 0.9513189471754464},\n",
      "'SnIIIms [1/l]': {'mean': 2.4440156651685347, 'sd': 44.02705373701091},\n",
      "'N2 Cet-Start [%]': {'mean': 78.71963071900763, 'sd': 0.5698597652353171},\n",
      "'N2 Cet Norm [%]': {'mean': 15.426211011264344, 'sd': 19.230296481082185},\n",
      "'N2 C mean slope': {'mean': 11.789620986270418, 'sd': 14.894094723646074},\n",
      "'N2 C mean breath': {'mean': 8.594905581729952, 'sd': 11.023586515408446},\n",
      "'VolInsp [l]': {'mean': 0.30181482172647867, 'sd': 0.15608622971075944},\n",
      "'VolExp [l]': {'mean': 0.29635537040883403, 'sd': 0.20077721360498088},\n",
      "'CEV [l]': {'mean': 7.3472713624810115, 'sd': 7.316445365782281},\n",
      "'CEV-DS [l]': {'mean': 6.327214485182281, 'sd': 6.515492731882587},\n",
      "'N2InspMean [%]': {'mean': 0.349038145152956, 'sd': 2.4698162052311963},\n",
      "'VolN2Exp [ml]': {'mean': 23.709167084685543, 'sd': 36.28571797674651},\n",
      "'VolN2Netto [ml]': {'mean': 21.321439981456233, 'sd': 33.706110364395364},\n",
      "'CumVolN2Netto [ml]': {'mean': 755.8622998842998, 'sd': 463.3701109129372},\n",
      "'VolN2Reinsp [ml]': {'mean': 2.3877271032293432, 'sd': 3.6880130884585878},\n",
      "'SIII': {'mean': 16.65984222208928, 'sd': 52.229273073959156},\n",
      "'SnIII, C breath*VT': {'mean': 0.39272426384099246, 'sd': 1.0910349010120886},\n",
      "'VdCO2 [ml]': {'mean': 53.44281869446865, 'sd': 227.96741664009454},\n",
      "'FlowInsp. mean [ml/s]': {'mean': -239.36576615573767, 'sd': 93.56796391020154},\n",
      "'FlowExp. mean [ml/s]': {'mean': 179.95988981343933, 'sd': 83.34569066272573},\n",
      "'RR': {'mean': 21.64155696315315, 'sd': 7.5519345888992895},\n",
      "'VolExp-DS [l]': {'mean': 0.25665370665630255, 'sd': 0.19633376357511967},\n",
      "'VolN2Netto filtered [ml]': {'mean': 2258.1189304211566, 'sd': 415434.1729929157},\n",
      "'VolN2Netto fast [ml]': {'mean': -216896968548445.78, 'sd': 5.8595217035909144e+16},\n",
      "'VdN2 [ml]': {'mean': -13844.694496650844, 'sd': 11483397.430097807},\n",
      "'VT alv. N2 [ml]': {'mean': 14105.698197679654, 'sd': 11483397.07122142},\n",
      "\n",
      "'Breath #': {'mean': 20.612465736836892, 'sd': 17.4036767011856},\n",
      "'Insp.Time [s]': {'mean': 1.279147136204992, 'sd': 0.6367608213042206},\n",
      "'Exp.Time [s]': {'mean': 1.692240717297266, 'sd': 0.6642403693250555},\n",
      "'Total breath time [s]': {'mean': 2.971387853502237, 'sd': 1.081041786058406},\n",
      "'PIF [ml/s]': {'mean': 345.9037274158502, 'sd': 145.89871065533916},\n",
      "'PEF [ml/s]': {'mean': 273.73590006486955, 'sd': 140.50924950613043},\n",
      "'Time to PIF [s]': {'mean': 0.6031245472859215, 'sd': 0.3423720758951037},\n",
      "'Time to PEF [s]': {'mean': 0.6675856166268545, 'sd': 0.5056139995649551},\n",
      "'Insp. Volume [ml]': {'mean': 296.9105147344029, 'sd': 174.58857337881972},\n",
      "'Exp. Volume [ml]': {'mean': 292.1540746427037, 'sd': 189.50423002605277},\n",
      "'EEL [ml]': {'mean': -4.756440091699739, 'sd': 159.55862462701342},\n",
      "'EEL cum. [ml]': {'mean': -164.39736698357615, 'sd': 1198.3613004133924},\n",
      "'Tidal Volume [ml]': {'mean': 294.53229468855346, 'sd': 163.8040686396198},\n",
      "'RR [1/min]': {'mean': 21.793569036866973, 'sd': 8.657622919360085},\n",
      "'Ratio Insp./Tot. Time [%]': {'mean': 43.02035064686235, 'sd': 9.459777625053826},\n",
      "'Ratio Exp./Tot. Time [%]': {'mean': 56.97964935313641, 'sd': 9.459777625053826},\n",
      "'Ratio Insp./Exp. Time [%]': {'mean': 82.65784428658499, 'sd': 66.37165639756282},\n",
      "'Ratio PEF/Exp. Time [%]': {'mean': 40.68106582006325, 'sd': 31.054035567278873},\n",
      "'MTIF [ml/s]': {'mean': 236.54650625143782, 'sd': 98.47708432593083},\n",
      "'MTEF [ml/s]': {'mean': 178.6105272764789, 'sd': 85.70199904676832},\n",
      "'Minute ventilation [ml/min]': {'mean': 5951.295052586274, 'sd': 4375.136328586452},\n",
      "'TEF75 [ml/s]': {'mean': 251.31058714873143, 'sd': 133.39264587079887},\n",
      "'TEF50 [ml/s]': {'mean': 247.4223987180923, 'sd': 127.84962589039982},\n",
      "'TEF25 [ml/s]': {'mean': 203.9850842831502, 'sd': 106.93819736448586},\n",
      "'TEF10 [ml/s]': {'mean': 155.5835719887861, 'sd': 86.34181943409651},\n",
      "'TIF50 [ml/s]': {'mean': 328.35236259384976, 'sd': 137.83291439631262},\n",
      "'VPIF [ml]': {'mean': 139.8800755204636, 'sd': 84.01537845502482},\n",
      "'VPEF [ml]': {'mean': 112.05779335651232, 'sd': 85.6939271307265},\n",
      "'TEF50/TIF50 [%]': {'mean': 888.7456356779902, 'sd': 255392.67335778946},\n",
      "'TEF75/PEF [%]': {'mean': 91.06451712248757, 'sd': 11.856439417007417},\n",
      "'TEF50/PEF [%]': {'mean': 90.10297034431913, 'sd': 10.579178373841415},\n",
      "'TEF25/PEF [%]': {'mean': 75.56078998602915, 'sd': 15.579501089716443},\n",
      "'TEF10/PEF [%]': {'mean': 58.75512871686148, 'sd': 18.112138062520803},\n",
      "'PEF/Exp.Vol. [1/s]': {'mean': 1.0722236641081282, 'sd': 0.6188764978719409},\n",
      "'VPEF/VT [%]': {'mean': 39.41022608756148, 'sd': 20.84161292911563},\n",
      "'AFV [l*l/s]': {'mean': 0.17255183553674996, 'sd': 1.0250750423956556},\n",
      "'VTinsp/Tinsp [ml/s]': {'mean': 233.54136287484806, 'sd': 98.92742781865704},\n",
      "'O2 consumed [ml]': {'mean': 29.25633437448992, 'sd': 127.58469379191799},\n",
      "'CO2 emitted [ml]': {'mean': 10.190173523334542, 'sd': 6.702696341080727},\n",
      "'RQ': {'mean': 0.9345499528155874, 'sd': 17.501856846826815},\n",
      "'et CO2 [%]': {'mean': 5.1212084266909415, 'sd': 0.5537068456060156},\n",
      "'et O2 [%]': {'mean': 61.98562575126697, 'sd': 33.534664801234456},\n",
      "\n",
      "'Date of birth': {'mean': 1255490489.9505765, 'sd': 77155130.76495366},\n",
      "'Height [cm]': {'mean': 122.89639929983524, 'sd': 35.3121603187434},\n",
      "'Weight [kg]': {'mean': 25.8375226523888, 'sd': 11.426642775164355},\n",
      "'Trial #': {'mean': 4.1834843492586495, 'sd': 2.913595070730993},\n",
      "'Washout time [s]': {'mean': 84.53894306013179, 'sd': 63.76092054529476},\n",
      "'# Washout Breaths': {'mean': 32.44759060955519, 'sd': 23.09982305360015},\n",
      "'FRC [l]': {'mean': 0.9025274257909011, 'sd': 4.155435751002455},\n",
      "'LCI-2.5': {'mean': 7.2437985670725595, 'sd': 3.5597797030190383},\n",
      "'LCI-5': {'mean': 4.9291788390208735, 'sd': 1.988627515358441},\n",
      "'Scond*VT': {'mean': 0.06782108742021475, 'sd': 0.6298171089253431},\n",
      "'Sacin*VT': {'mean': 0.11499371742895963, 'sd': 0.20865004862094014},\n",
      "'Pacin*VT': {'mean': 0.23264732764508453, 'sd': 2.252412575798894},\n",
      "'Scond [1/l]': {'mean': 1.218530411963783, 'sd': 33.84957336219557},\n",
      "'Sacin [1/l]': {'mean': 0.7689938988579224, 'sd': 7.316833464472154},\n",
      "'Pacin [1/l]': {'mean': 2.7470651841204345, 'sd': 22.64402802547439},\n",
      "'1st breath SnIII*VT': {'mean': 0.1252181142726818, 'sd': 0.255638790356801},\n",
      "'1st breath SnIII [1/l]': {'mean': 0.9910715155585816, 'sd': 8.23439465075105},\n",
      "'FidN2': {'mean': 0.31074805770930447, 'sd': 0.30829106639186393},\n",
      "'VdF/VT [%]': {'mean': 3.3959055038022314, 'sd': 10063.143884746964},\n",
      "'W faster': {'mean': 0.7276658563938, 'sd': 0.36353881345134836},\n",
      "'W slower': {'mean': 0.7686696332187205, 'sd': 0.337129664871148},\n",
      "'W full': {'mean': 0.7651593977537848, 'sd': 0.3248260021203599},\n",
      "'VT alv. faster [ml]': {'mean': -281435172.1588522, 'sd': 21593618315.283924},\n",
      "'VT alv. slower [ml]': {'mean': 303849569.760336, 'sd': 25435169185.52802},\n",
      "'VT alv. full [ml]': {'mean': 84.32194926886616, 'sd': 71.06478356145108},\n",
      "'FRC faster / FRC full [%]': {'mean': -13529297.373725457, 'sd': 1147575912.3387156},\n",
      "'FRC slower / FRC full [%]': {'mean': 13529379.945590477, 'sd': 1147575912.544213},\n",
      "'Specific ventilation faster [%]': {'mean': 8.80291436454543, 'sd': 9.348819682567317},\n",
      "'Specific ventilation slower [%]': {'mean': 9.046985684854585, 'sd': 7.894490953080439},\n",
      "'Specific ventilation ratio': {'mean': 0.9581156774935863, 'sd': 1.0819544042294191},\n",
      "'FRC faster [ml]': {'mean': -253972130.0778201, 'sd': 21534177573.593132},\n",
      "'FRC slower [ml]': {'mean': 253972925.73823825, 'sd': 21534177586.35868},\n",
      "'FRC full [ml]': {'mean': 794.8412258085103, 'sd': 704.0171710490173},\n",
      "'VT alv. N2 mean [ml]': {'mean': 19514.010313795457, 'sd': 2175143.8489836883},\n",
      "'M1/M0': {'mean': 1.4617309155019946, 'sd': 0.9079510271116188},\n",
      "'M2/M0': {'mean': 6.269320996537113, 'sd': 7.160121647731635},\n",
      "'M1/M0-6': {'mean': 1.1537074290878406, 'sd': 1.0033155973918328},\n",
      "'M2/M0-6': {'mean': 3.1229648592814274, 'sd': 5.401518246632201},\n",
      "'M1/M0-8': {'mean': 1.2922468742617355, 'sd': 0.7431887203671709},\n",
      "'M2/M0-8': {'mean': 4.243888069795803, 'sd': 4.026955390887476},\n",
      "'CEV [l]': {'mean': 8.345798896711049, 'sd': 8.23330964067737},\n",
      "'N2 Cet-Start [%]': {'mean': 77.64680708066874, 'sd': 9.036156155027863},\n",
      "'Flow Insp. mean [ml/s]': {'mean': 199.24452480526853, 'sd': 101.49574748840872},\n",
      "'Flow Exp. mean [ml/s]': {'mean': 150.4268729932062, 'sd': 84.37006828571742},\n",
      "'VT Insp. mean [ml]': {'mean': 254.42805429356562, 'sd': 153.12520905139215},\n",
      "'VT Exp. mean [ml]': {'mean': 248.96270962377736, 'sd': 159.41763015491932},\n",
      "'VT mean [ml]': {'mean': 251.6953819587858, 'sd': 152.98498621059176},\n",
      "'RQ': {'mean': 0.9029343771985184, 'sd': 1.0758361297096963},\n",
      "'VT mean/FRC': {'mean': 0.25939464936803897, 'sd': 1.2343402630699176},\n",
      "'N2Cet norm @ TO6 [%]': {'mean': 3.1168939791612317e+99, 'sd': 2.0252616612979062e+101},\n",
      "'Vd CO2 mean [ml]': {'mean': 42.12186006703125, 'sd': 59.4072715170959},\n",
      "'et CO2 mean [%]': {'mean': 4.507970175277892, 'sd': 1.7844732477230152},\n",
      "\n",
      "'time': {'mean': 88.55432734935935, 'sd': 66.25471915520497},\n",
      "'val': {'mean': 68.23874450332863, 'sd': 35.26258583001278},\n",
      "\n",
      "'time': {'mean': 88.63540194748052, 'sd': 66.1581685099236},\n",
      "'val': {'mean': 28.146122014093592, 'sd': 34.40387038121169},\n",
      "\n",
      "'time': {'mean': 89.60838121866202, 'sd': 67.59353031114709},\n",
      "'val': {'mean': 2.196155238050134, 'sd': 2.153426417738841},\n",
      "\n",
      "'time': {'mean': 88.62542944789226, 'sd': 66.165372425371},\n",
      "'val': {'mean': -42.94877586296252, 'sd': 355.98709350315255},\n",
      "\n",
      "'time': {'mean': 88.60362842217994, 'sd': 66.1579923954802},\n",
      "'val': {'mean': -26.19616285442927, 'sd': 964.8809401397783},\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for feature_df in [breath_mean_sd_df, tbfvl_mean_sd_df, spx_mean_sd_df]:\n",
    "    for index, row in feature_df.iterrows():\n",
    "        print(\n",
    "            \"'{}': {'mean': {}, 'sd': {}},\".format(\n",
    "                row['index'], str(row[0]), str(row[1])\n",
    "            )\n",
    "        )\n",
    "    print(\"\")\n",
    "\n",
    "for signal in ['o2', 'n2', 'co2', 'flow', 'volume']:\n",
    "    for index, row in ss_dict[signal].iterrows():\n",
    "        print(\n",
    "            \"'{}': {'mean': {}, 'sd': {}},\".format(\n",
    "                row['index'], str(row[0]), str(row[1])\n",
    "            )\n",
    "        )\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.iv: Load and Process Data<a name=\"section-2iv-load-process\"></a>\n",
    "At this point, we have a dataframe that contain the labels and the location of the feature files. We also now have information necessary to process the data. It is time to now to create our datasets used by TensorFlow. We will take the location of the feature files, apply our functions to preprocess the data, and store the data in the TFRecord format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = '../data/raw/'\n",
    "INTERMEDIARY_PATH = '../data/intermediary/'\n",
    "PROCESSED_PATH = '../data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "redcap_qc = pd.read_csv(\n",
    "    os.path.join(INTERMEDIARY_PATH, 'main_id_associated_files.csv'), \n",
    "    keep_default_na=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all columns were available for all trials. It was therefore necessary to define which columns would be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BREATH = [\n",
    "    'N2 Cet [%]', 'TO', 'FRC [l]', 'SnIIIms [1/l]', 'N2 Cet-Start [%]',\n",
    "    'N2 Cet Norm [%]', 'N2 C mean slope', 'N2 C mean breath', 'VolInsp [l]',\n",
    "    'VolExp [l]', 'CEV [l]', 'CEV-DS [l]', 'N2InspMean [%]', 'VolN2Exp [ml]',\n",
    "    'VolN2Netto [ml]', 'CumVolN2Netto [ml]', 'VolN2Reinsp [ml]', 'SIII',\n",
    "    'SnIII, C breath*VT', 'VdCO2 [ml]', 'FlowInsp. mean [ml/s]',\n",
    "    'FlowExp. mean [ml/s]', 'RR', 'VolExp-DS [l]', 'VolN2Netto filtered [ml]',\n",
    "    'VolN2Netto fast [ml]', 'VdN2 [ml]', 'VT alv. N2 [ml]',\n",
    "]\n",
    "\n",
    "TBFVL = [\n",
    "    'Insp.Time [s]', 'Exp.Time [s]', 'Total breath time [s]', 'PIF [ml/s]',\n",
    "    'PEF [ml/s]', 'Time to PIF [s]', 'Time to PEF [s]', 'Insp. Volume [ml]',\n",
    "    'Exp. Volume [ml]', 'EEL [ml]', 'EEL cum. [ml]', 'Tidal Volume [ml]',\n",
    "    'RR [1/min]', 'Ratio Insp./Tot. Time [%]', 'Ratio Exp./Tot. Time [%]',\n",
    "    'Ratio Insp./Exp. Time [%]', 'Ratio PEF/Exp. Time [%]', 'MTIF [ml/s]',\n",
    "    'MTEF [ml/s]', 'Minute ventilation [ml/min]', 'TEF75 [ml/s]',\n",
    "    'TEF50 [ml/s]', 'TEF25 [ml/s]', 'TEF10 [ml/s]', 'TIF50 [ml/s]', 'VPIF [ml]',\n",
    "    'VPEF [ml]', 'TEF50/TIF50 [%]', 'TEF75/PEF [%]', 'TEF50/PEF [%]',\n",
    "    'TEF25/PEF [%]', 'TEF10/PEF [%]', 'PEF/Exp.Vol. [1/s]', 'VPEF/VT [%]',\n",
    "    'AFV [l*l/s]', 'VTinsp/Tinsp [ml/s]', 'O2 consumed [ml]', \n",
    "    'CO2 emitted [ml]', 'RQ', 'et CO2 [%]', 'et O2 [%]', 'W', 'P'\n",
    "]\n",
    "\n",
    "SPX = [\n",
    "    'Date of birth', 'Height [cm]', 'Weight [kg]', 'Trial #',\n",
    "    'Washout time [s]', '# Washout Breaths', 'FRC [l]', 'LCI-2.5', 'LCI-5',\n",
    "    'FidN2', 'VdF/VT [%]', 'W faster', 'W slower', 'W full',\n",
    "    'VT alv. faster [ml]', 'VT alv. slower [ml]', 'VT alv. full [ml]',\n",
    "    'FRC faster / FRC full [%]', 'FRC slower / FRC full [%]',\n",
    "    'Specific ventilation faster [%]', 'Specific ventilation slower [%]',\n",
    "    'Specific ventilation ratio', 'FRC faster [ml]', 'FRC slower [ml]',\n",
    "    'FRC full [ml]', 'VT alv. N2 mean [ml]', 'M1/M0', 'M2/M0', 'M1/M0-6',\n",
    "    'M2/M0-6', 'M1/M0-8', 'M2/M0-8', 'CEV [l]', 'N2 Cet-Start [%]',\n",
    "    'Flow Insp. mean [ml/s]', 'Flow Exp. mean [ml/s]', 'VT Insp. mean [ml]',\n",
    "    'VT Exp. mean [ml]', 'VT mean [ml]', 'RQ', 'VT mean/FRC',\n",
    "    'N2Cet norm @ TO6 [%]', 'Vd CO2 mean [ml]', 'et CO2 mean [%]', 'Male', 'Female'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Section 2.iii, we obtained information used in padding and standardization. We can now define some functions that will pad and standardize the data incorporating the information from Section 2.iii. We also define a function that will create a boolean column to indicate missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_rows(raw_table, n_rows):\n",
    "    \"\"\"Pad/add additional rows to dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_table : pandas.dataframe\n",
    "        Dataframe of interest\n",
    "    n_rows : int\n",
    "        Number of rows to add to raw_table\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.dataframe\n",
    "        Dataframe with padded rows of 0\n",
    "    \"\"\"\n",
    "    padded_table = raw_table.reindex(range(n_rows)).fillna(0)\n",
    "\n",
    "    return padded_table\n",
    "\n",
    "def standardize(raw_table, mean_sd_dict):\n",
    "    for raw_table_cols in raw_table.columns.values:\n",
    "\n",
    "        feature_mean = mean_sd_dict[raw_table_cols]['mean']\n",
    "        feature_sd = mean_sd_dict[raw_table_cols]['sd']\n",
    "\n",
    "        raw_table[raw_table_cols] = (\n",
    "            (raw_table[raw_table_cols] - feature_mean)/(feature_sd)\n",
    "        )\n",
    "\n",
    "    # subset raw table using only the keys from dictionary\n",
    "    # final_table should only contain z-scores\n",
    "    standardize_table = raw_table[raw_table.columns.values]\n",
    "\n",
    "    return standardize_table\n",
    "\n",
    "def create_bool_col(raw_table, skip_col=None):\n",
    "    \"\"\"Create boolean columns which correspond to missing values\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_table : pandas.dataframe\n",
    "        [description]\n",
    "    skip_col : list of strings, optional\n",
    "        Names of columns to skip in raw_table, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    raw_table_cols = raw_table.columns.values.tolist()\n",
    "\n",
    "    if skip_col != None:\n",
    "        raw_table_cols = [col for col in raw_table_cols if col not in skip_col]\n",
    "\n",
    "    for col in raw_table_cols:\n",
    "        raw_table['{}_bool'.format(col)] = raw_table[col].notna().astype(int)\n",
    "\n",
    "    return raw_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_breath(breath_table):\n",
    "    \"\"\"Process single breath table\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    breath_table : pandas.dataframe\n",
    "        The dataframe to be processed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.dataframe\n",
    "        Breath table that is cleaned, standardized, padded, with a boolean\n",
    "        column to indicate missing values\n",
    "    \"\"\"\n",
    "    breath_table.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    try:\n",
    "        breath_table = breath_table.drop(columns = [\n",
    "            'Excluded for Sacin/Scond calculation', 'VdCO2 Langley [ml]'\n",
    "        ])\n",
    "        breath_table = breath_table.rename(\n",
    "            columns={'VdCO2 Fowler [ml]': 'VdCO2 [ml]'}\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    breath_table = breath_table.drop(columns = ['Breath #'])\n",
    "    \n",
    "    # obtained from Section 2.iii; used to standardize data\n",
    "    breath_dict={\n",
    "        'N2 Cet [%]': {'mean': 12.153882738375403, 'sd': 15.146946095000398},\n",
    "        'TO': {'mean': 5.373229399786835, 'sd': 3.4905444722306527},\n",
    "        'FRC [l]': {'mean': 1.0858187396262766, 'sd': 0.9541204193329395},\n",
    "        'SnIIIms [1/l]': {'mean': 2.436704496635718, 'sd': 44.172888433741115},\n",
    "        'N2 Cet-Start [%]': {'mean': 78.71975309865458, 'sd': 0.5702802883948145},\n",
    "        'N2 Cet Norm [%]': {'mean': 15.437814726560148, 'sd': 19.237251687090218},\n",
    "        'N2 C mean slope': {'mean': 11.7981740544857, 'sd': 14.899463233593318},\n",
    "        'N2 C mean breath': {'mean': 8.600724857941282, 'sd': 11.027203854199875},\n",
    "        'VolInsp [l]': {'mean': 0.3024531561143202, 'sd': 0.15620154067731323},\n",
    "        'VolExp [l]': {'mean': 0.29696582278173644, 'sd': 0.20118360135868985},\n",
    "        'CEV [l]': {'mean': 7.368848459186139, 'sd': 7.334883401302143},\n",
    "        'CEV-DS [l]': {'mean': 6.345877225078502, 'sd': 6.532314023147374},\n",
    "        'N2InspMean [%]': {'mean': 0.3476471823217033, 'sd': 2.4687831098982387},\n",
    "        'VolN2Exp [ml]': {'mean': 23.762126946384935, 'sd': 36.342708423428164},\n",
    "        'VolN2Netto [ml]': {'mean': 21.371505963447053, 'sd': 33.760733010916375},\n",
    "        'CumVolN2Netto [ml]': {'mean': 758.0587540776661, 'sd': 464.2319361660444},\n",
    "        'VolN2Reinsp [ml]': {'mean': 2.3906209829378824, 'sd': 3.694096528988632},\n",
    "        'SIII': {'mean': 16.62026849119295, 'sd': 52.0866479377276},\n",
    "        'SnIII, C breath*VT': {'mean': 0.3929482248530059, 'sd': 1.0926357552056198},\n",
    "        'VdCO2 [ml]': {'mean': 53.52361250159101, 'sd': 228.6797951540405},\n",
    "        'FlowInsp. mean [ml/s]': {'mean': -239.7494503273387, 'sd': 93.56379023866181},\n",
    "        'FlowExp. mean [ml/s]': {'mean': 180.1956568715721, 'sd': 83.36041837364677},\n",
    "        'RR': {'mean': 21.627064533951323, 'sd': 7.543466199691876},\n",
    "        'VolExp-DS [l]': {'mean': 0.2571750119173467, 'sd': 0.1967484970715272},\n",
    "        'VolN2Netto filtered [ml]': {'mean': 2274.8835729392886, 'sd': 416984.9262282386},\n",
    "        'VolN2Netto fast [ml]': {'mean': -218576995892672.9, 'sd': 5.882170856957783e+16},\n",
    "        'VdN2 [ml]': {'mean': -13948.381121424547, 'sd': 11526257.13744747},\n",
    "        'VT alv. N2 [ml]': {'mean': 14209.873164337721, 'sd': 11526256.776634963},\n",
    "    }\n",
    "    \n",
    "    breath_table = standardize(breath_table, breath_dict)\n",
    "    breath_table = create_bool_col(breath_table)\n",
    "    breath_table = pad_rows(breath_table, 187)\n",
    "\n",
    "    return breath_table\n",
    "\n",
    "def process_tbfvl(tbfvl_table):\n",
    "    \"\"\"Process single TBFVL table\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tbfvl_table : pandas.dataframe\n",
    "        The dataframe to be processed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.dataframe\n",
    "        TBFVL table that is cleaned, standardized, padded, with a boolean\n",
    "        column to indicate missing values\n",
    "    \"\"\"\n",
    "    tbfvl_table.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    tbfvl_table_dum = pd.get_dummies(tbfvl_table['Phase'])\n",
    "    if not 'W' in tbfvl_table_dum.columns:\n",
    "        tbfvl_table_dum['W'] = 0\n",
    "    if not 'P' in tbfvl_table_dum.columns:\n",
    "        tbfvl_table_dum['P'] = 0\n",
    "    tbfvl_table_dum = tbfvl_table_dum[['P', 'W']]\n",
    "    \n",
    "    tbfvl_table = tbfvl_table.drop(\n",
    "        columns = ['Phase', 'Breath #', 'Timestamp (UTC)']\n",
    "    )\n",
    "\n",
    "    # obtained from Section 2.iii; used to standardize data\n",
    "    tbfvl_dict={\n",
    "        'Insp.Time [s]': {'mean': 1.2797538928784322, 'sd': 0.6363674460388641},\n",
    "        'Exp.Time [s]': {'mean': 1.6933594892170798, 'sd': 0.6640419329601139},\n",
    "        'Total breath time [s]': {'mean': 2.9731133820954883, 'sd': 1.080811473993286},\n",
    "        'PIF [ml/s]': {'mean': 346.41345521990553, 'sd': 145.94254693899146},\n",
    "        'PEF [ml/s]': {'mean': 274.0723334710935, 'sd': 140.59437380117248},\n",
    "        'Time to PIF [s]': {'mean': 0.6032923247234108, 'sd': 0.34241641955579544},\n",
    "        'Time to PEF [s]': {'mean': 0.6677677983135155, 'sd': 0.5051787708426468},\n",
    "        'Insp. Volume [ml]': {'mean': 297.521038949005, 'sd': 174.8415245099567},\n",
    "        'Exp. Volume [ml]': {'mean': 292.7281631780046, 'sd': 189.86628049098528},\n",
    "        'EEL [ml]': {'mean': -4.792875771000758, 'sd': 160.02623802114027},\n",
    "        'EEL cum. [ml]': {'mean': -165.78904804781897, 'sd': 1202.541203976086},\n",
    "        'Tidal Volume [ml]': {'mean': 295.124601063505, 'sd': 164.03439385209424},\n",
    "        'RR [1/min]': {'mean': 21.778889891023105, 'sd': 8.646669811816205},\n",
    "        'Ratio Insp./Tot. Time [%]': {'mean': 43.01587195715785, 'sd': 9.443479736517506},\n",
    "        'Ratio Exp./Tot. Time [%]': {'mean': 56.98412804284092, 'sd': 9.443479736517506},\n",
    "        'Ratio Insp./Exp. Time [%]': {'mean': 82.61291770040698, 'sd': 66.35908419531079},\n",
    "        'Ratio PEF/Exp. Time [%]': {'mean': 40.659256280378564, 'sd': 30.97466072212236},\n",
    "        'MTIF [ml/s]': {'mean': 236.8927543675094, 'sd': 98.50483303756096},\n",
    "        'MTEF [ml/s]': {'mean': 178.82293979782767, 'sd': 85.72743995849184},\n",
    "        'Minute ventilation [ml/min]': {'mean': 5959.570739235493, 'sd': 4386.050686273252},\n",
    "        'TEF75 [ml/s]': {'mean': 251.62685872209016, 'sd': 133.4192342201182},\n",
    "        'TEF50 [ml/s]': {'mean': 247.73133034663553, 'sd': 127.89648521975207},\n",
    "        'TEF25 [ml/s]': {'mean': 204.20211195639675, 'sd': 106.99888837092331},\n",
    "        'TEF10 [ml/s]': {'mean': 155.70819128243988, 'sd': 86.41135066694149},\n",
    "        'TIF50 [ml/s]': {'mean': 328.8390334997921, 'sd': 137.85935839822903},\n",
    "        'VPIF [ml]': {'mean': 140.12219503186364, 'sd': 84.08025056339659},\n",
    "        'VPEF [ml]': {'mean': 112.2833324539225, 'sd': 85.8482249837884},\n",
    "        'TEF50/TIF50 [%]': {'mean': 891.6328732699907, 'sd': 256366.21199885794},\n",
    "        'TEF75/PEF [%]': {'mean': 91.07214464273777, 'sd': 11.835260521315465},\n",
    "        'TEF50/PEF [%]': {'mean': 90.10795074918406, 'sd': 10.567776696055914},\n",
    "        'TEF25/PEF [%]': {'mean': 75.54909539549756, 'sd': 15.576441486292023},\n",
    "        'TEF10/PEF [%]': {'mean': 58.72707472051545, 'sd': 18.101581522472163},\n",
    "        'PEF/Exp.Vol. [1/s]': {'mean': 1.07114594132629, 'sd': 0.6180617273977341},\n",
    "        'VPEF/VT [%]': {'mean': 39.40238099155333, 'sd': 20.81656112805978},\n",
    "        'AFV [l*l/s]': {'mean': 0.17308893777746165, 'sd': 1.0289346569443358},\n",
    "        'VTinsp/Tinsp [ml/s]': {'mean': 233.88182082354655, 'sd': 98.95444571414659},\n",
    "        'O2 consumed [ml]': {'mean': 29.333572081442092, 'sd': 127.95411766895819},\n",
    "        'CO2 emitted [ml]': {'mean': 10.211686279029067, 'sd': 6.711886962072948},\n",
    "        'RQ': {'mean': 0.9347406301193033, 'sd': 17.567854972313437},\n",
    "        'et CO2 [%]': {'mean': 5.122768724878801, 'sd': 0.5532392910456915},\n",
    "        'et O2 [%]': {'mean': 61.97589131675811, 'sd': 33.532385379981235},\n",
    "    }\n",
    "\n",
    "    tbfvl_table = standardize(tbfvl_table, tbfvl_dict)\n",
    "    tbfvl_table = pd.concat(\n",
    "        [tbfvl_table.reset_index(drop=True), tbfvl_table_dum], axis=1\n",
    "    )\n",
    "    tbfvl_table = create_bool_col(tbfvl_table)\n",
    "    tbfvl_table = pad_rows(tbfvl_table, 203)\n",
    "\n",
    "    return tbfvl_table\n",
    "\n",
    "def process_spx(spx_df):\n",
    "    \"\"\"Process single spx\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spx_df : pandas.dataframe\n",
    "        The dataframe to be processed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.dataframe\n",
    "        SPX dataframe that is cleaned, standardized, padded, with a boolean\n",
    "        column to indicate missing values\n",
    "    \"\"\"\n",
    "    spx_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # obtained from Section 2.iii; used to standardize data\n",
    "    spx_dict={\n",
    "        'Date of birth': {'mean': 1255353813.8860104, 'sd': 77344813.94431767},\n",
    "        'Height [cm]': {'mean': 123.02784766839378, 'sd': 35.381900798194565},\n",
    "        'Weight [kg]': {'mean': 25.898219689119173, 'sd': 11.436210456645634},\n",
    "        'Trial #': {'mean': 4.181450777202072, 'sd': 2.912964856333764},\n",
    "        'Washout time [s]': {'mean': 84.50111450777202, 'sd': 63.912161320303085},\n",
    "        '# Washout Breaths': {'mean': 32.407046632124356, 'sd': 23.136903741207938},\n",
    "        'FRC [l]': {'mean': 0.9037610567927414, 'sd': 4.168707549575682},\n",
    "        'LCI-2.5': {'mean': 7.23461735138295, 'sd': 3.5658937036862435},\n",
    "        'LCI-5': {'mean': 4.9238518879363635, 'sd': 1.9927743515469327},\n",
    "        'FidN2': {'mean': 0.31119971174158695, 'sd': 0.309218475498321},\n",
    "        'VdF/VT [%]': {'mean': 3.054105593110619, 'sd': 10099.663484440092},\n",
    "        'W faster': {'mean': 0.7265870278816335, 'sd': 0.36449056345170167},\n",
    "        'W slower': {'mean': 0.7680085543316435, 'sd': 0.3380721794524283},\n",
    "        'W full': {'mean': 0.7644553735920073, 'sd': 0.3256802049735196},\n",
    "        'VT alv. faster [ml]': {'mean': -283748734.15008336, 'sd': 21682192749.576374},\n",
    "        'VT alv. slower [ml]': {'mean': 306337992.4861179, 'sd': 25539109422.600826},\n",
    "        'VT alv. full [ml]': {'mean': 84.52938464882929, 'sd': 71.22668060754128},\n",
    "        'FRC faster / FRC full [%]': {'mean': -13644883.865959585, 'sd': 1152467596.0237799},\n",
    "        'FRC slower / FRC full [%]': {'mean': 13644966.302934375, 'sd': 1152467596.2317512},\n",
    "        'Specific ventilation faster [%]': {'mean': 8.774090131798937, 'sd': 9.364790710266577},\n",
    "        'Specific ventilation slower [%]': {'mean': 9.006844446364664, 'sd': 7.89722921185944},\n",
    "        'Specific ventilation ratio': {'mean': 0.9588034971418548, 'sd': 1.0859257171938819},\n",
    "        'FRC faster [ml]': {'mean': -256141920.24584636, 'sd': 21625969648.396786},\n",
    "        'FRC slower [ml]': {'mean': 256142719.0246412, 'sd': 21625969661.17982},\n",
    "        'FRC full [ml]': {'mean': 797.9417475990174, 'sd': 705.995209510256},\n",
    "        'VT alv. N2 mean [ml]': {'mean': 19639.013292178577, 'sd': 2182165.383269265},\n",
    "        'M1/M0': {'mean': 1.457213483160203, 'sd': 0.8935620055758221},\n",
    "        'M2/M0': {'mean': 6.2429303824395515, 'sd': 7.082424679845044},\n",
    "        'M1/M0-6': {'mean': 1.1437500933063385, 'sd': 0.6156695409978187},\n",
    "        'M2/M0-6': {'mean': 3.07362169269162, 'sd': 3.2769037503294145},\n",
    "        'M1/M0-8': {'mean': 1.2876635170531843, 'sd': 0.709284473605157},\n",
    "        'M2/M0-8': {'mean': 4.221848493457361, 'sd': 3.8156767434547034},\n",
    "        'CEV [l]': {'mean': 8.354690340015287, 'sd': 8.256338563387251},\n",
    "        'N2 Cet-Start [%]': {'mean': 77.6401703260693, 'sd': 9.064655291744732},\n",
    "        'Flow Insp. mean [ml/s]': {'mean': 199.3130596501723, 'sd': 101.73035209256297},\n",
    "        'Flow Exp. mean [ml/s]': {'mean': 150.4402457097296, 'sd': 84.5176186736973},\n",
    "        'VT Insp. mean [ml]': {'mean': 254.60975300784006, 'sd': 153.44963333195182},\n",
    "        'VT Exp. mean [ml]': {'mean': 249.120262279826, 'sd': 159.786142750936},\n",
    "        'VT mean [ml]': {'mean': 251.86500764394808, 'sd': 153.3183014405259},\n",
    "        'RQ': {'mean': 0.9029283969167898, 'sd': 1.0792542484393604},\n",
    "        'VT mean/FRC': {'mean': 0.2590167139748991, 'sd': 1.2382610542745873},\n",
    "        'N2Cet norm @ TO6 [%]': {'mean': 3.139571762168011e+99, 'sd': 2.0326150948725518e+101},\n",
    "        'Vd CO2 mean [ml]': {'mean': 42.1308592778324, 'sd': 59.58611484255497},\n",
    "        'et CO2 mean [%]': {'mean': 4.505687988198351, 'sd': 1.7889436652717987},\n",
    "    }\n",
    "\n",
    "    spx_df_dum = pd.get_dummies(spx_df['Gender'])\n",
    "    if not 'Male' in spx_df_dum.columns:\n",
    "        spx_df_dum['Male'] = 0\n",
    "    if not 'Female' in spx_df_dum.columns:\n",
    "        spx_df_dum['Female'] = 0\n",
    "    spx_df_dum = spx_df_dum[['Male', 'Female']]\n",
    "\n",
    "    spx_df['Date of birth'] = (\n",
    "        pd.to_datetime(spx_df['Date of birth'], format='%d.%m.%Y')\n",
    "        - pd.Timestamp('1970-01-01')\n",
    "    ) // pd.Timedelta('1s')\n",
    "\n",
    "    spx_df = spx_df.drop(columns = [\n",
    "        'Patient-ID', 'Lastname', 'Firstname', 'Gender', \n",
    "        'Ethnicity', 'Smoker', 'Asthma', 'Notes', 'Test Date (UTC)',\n",
    "        'Timestamp (UTC)', 'Comment', 'FRC @ TO6 [l]', 'Scond*VT', 'Sacin*VT',\n",
    "        'Pacin*VT', 'Scond [1/l]',\t'Sacin [1/l]',\t'Pacin [1/l]',\n",
    "        '1st breath SnIII*VT',\t'1st breath SnIII [1/l]', 'Scond*VT [l]',\n",
    "        'Sacin*VT [l]', 'Pacin*VT [l]', 'Scond', 'Sacin','Pacin',\n",
    "    ], errors='ignore')\n",
    "\n",
    "    spx_df = standardize(spx_df, spx_dict)\n",
    "    spx_df = pd.concat([spx_df.reset_index(drop=True), spx_df_dum], axis=1)\n",
    "    spx_df = create_bool_col(spx_df)\n",
    "    spx_df = pad_rows(spx_df , 1)\n",
    "\n",
    "    return spx_df\n",
    "\n",
    "def process_screenshot(screenshot, ss_type):\n",
    "    \"\"\"Process single screenshot type\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    screenshot : pandas.dataframe\n",
    "        The dataframe to be processed\n",
    "    ss_type : str\n",
    "        Raw MBW signal to be processed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.dataframe\n",
    "        Screenshot data that is cleaned, standardized, padded, with a boolean\n",
    "        column to indicate missing values\n",
    "    \"\"\"\n",
    "\n",
    "    # obtained from Section 2.iii; used to standardize data\n",
    "    screenshot_dict = {\n",
    "        'o2':  {'mean': 68.23157418109633, 'sd': 35.262795056959945},\n",
    "        'co2': {'mean': 2.196753669014523, 'sd': 2.153852461462564},\n",
    "        'flow': {'mean': -42.97922884856061, 'sd': 356.93793207438284},\n",
    "        'n2': {'mean': 28.151857103277152, 'sd': 34.40444632705889},\n",
    "        'volume': {'mean': -26.96673191012796, 'sd': 967.9686423120496}\n",
    "    }\n",
    "\n",
    "    screenshot = interpolate_screenshot(screenshot)\n",
    "    screenshot.rename(columns={'val': ss_type}, inplace=True)\n",
    "    screenshot = screenshot.drop(columns=['time'])\n",
    "    screenshot = standardize(screenshot, screenshot_dict)\n",
    "    screenshot = pad_rows(screenshot, 5748)\n",
    "\n",
    "    return screenshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the training was done on a corporate computer with limited RAM, there was a potential the data set would not fit on memory. Therefore, it was necesary to create a data set using the TFRecord format. The functions below help with the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "# types need to be uniform in tensor\n",
    "def int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the functions defined to create our TFRecords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each TFRecord file will have a maximum of 1000 records\n",
    "max_examples_rec = 1000\n",
    "\n",
    "# we want separate data sets for train, validate and test\n",
    "for group_type in ['train', 'validate', 'test']:\n",
    "    redcap_split = redcap_qc.loc[redcap_qc['split_group'] == group_type]\n",
    "    redcap_split = redcap_split.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    max_files = (redcap_split.shape[0]//max_examples_rec) + 1\n",
    "\n",
    "    for tfrec_num in range(1, max_files+1):\n",
    "        rec_start = (tfrec_num - 1) * max_examples_rec\n",
    "        rec_stop = (tfrec_num) * max_examples_rec\n",
    "        if rec_stop > redcap_split.shape[0]:\n",
    "            redcap_split_sub = redcap_split.iloc[rec_start:]\n",
    "        else:\n",
    "            redcap_split_sub = redcap_split.iloc[rec_start:rec_stop]\n",
    "\n",
    "        with tf.io.TFRecordWriter(\n",
    "                PROCESSED_PATH + \"/\" + group_type + \"/\" \n",
    "                + group_type + \"_%.2i.tfrec\" % (tfrec_num)\n",
    "            ) as writer:\n",
    "\n",
    "            for single_record in zip(\n",
    "                redcap_split_sub['o2_path'], redcap_split_sub['n2_path'],  \n",
    "                redcap_split_sub['co2_path'],  redcap_split_sub['flow_path'],\n",
    "                redcap_split_sub['volume_path'], \n",
    "                redcap_split_sub['breath_path'], redcap_split_sub['tbfvl_path'],\n",
    "                redcap_split_sub['spx_export_path'], \n",
    "                redcap_split_sub['qc_grade_label'],\n",
    "                redcap_split_sub['trial_accepted_label']\n",
    "            ):\n",
    "                (\n",
    "                    o2_path, n2_path, co2_path, flow_path, \n",
    "                    volume_path, \n",
    "                    breath_path, tbfvl_path, spx_path,\n",
    "                    qc_grade_label,\n",
    "                    trial_accepted_label\n",
    "                ) = single_record\n",
    "\n",
    "                # used to store all data before being processed as a tf.record\n",
    "                feature = {}\n",
    "\n",
    "                # process the screenshot data\n",
    "                screenshot_path_dict = {\n",
    "                    'o2': o2_path,\n",
    "                    'co2': co2_path, \n",
    "                    'n2': n2_path,\n",
    "                    'flow': flow_path,\n",
    "                    'volume': volume_path,\n",
    "                }\n",
    "                for key, val in screenshot_path_dict.items():\n",
    "                    try:\n",
    "                        screenshot = pd.read_csv(\n",
    "                            val, header=None, delim_whitespace=True\n",
    "                        )\n",
    "                    except pd.errors.EmptyDataError:\n",
    "                        # empty dataframe if screenshot is empty due to white \n",
    "                        # screenshot\n",
    "                        screenshot = pd.DataFrame({'time':[0],'val':[0]})\n",
    "\n",
    "                    processed_screenshot = process_screenshot(screenshot, key)\n",
    "                    feature[key] = float_feature(\n",
    "                        processed_screenshot[key].astype(float).tolist()\n",
    "                    )\n",
    "\n",
    "                # process breath table\n",
    "                breath = pd.read_csv(breath_path, sep='\\t')\n",
    "                breath = process_breath(breath)\n",
    "                i = 1\n",
    "                for breath_col in BREATH:\n",
    "                    feature['breath_{}'.format(str(i))] = float_feature(\n",
    "                        breath[breath_col].tolist()\n",
    "                    )\n",
    "                    feature['breath_{}_bool'.format(str(i))] = float_feature(\n",
    "                        breath[breath_col+'_bool'].astype(float).tolist()\n",
    "                    )\n",
    "                    i += 1\n",
    "\n",
    "                # process tbfvl table\n",
    "                tbfvl = pd.read_csv(tbfvl_path, sep='\\t')\n",
    "                tbfvl = process_tbfvl(tbfvl)\n",
    "                i = 1\n",
    "                for tbfvl_col in TBFVL:\n",
    "                    feature['tbfvl_{}'.format(str(i))] = float_feature(\n",
    "                        tbfvl[tbfvl_col].tolist()\n",
    "                    )\n",
    "                    feature['tbfvl_{}_bool'.format(str(i))] = float_feature(\n",
    "                        tbfvl[tbfvl_col+'_bool'].astype(float).tolist()\n",
    "                    )\n",
    "                    i += 1\n",
    "\n",
    "                # process spx data\n",
    "                spx = pd.read_csv(spx_path)\n",
    "                spx = process_spx(spx)\n",
    "                i = 1\n",
    "                for spx_col in SPX:\n",
    "                    # spx has to be processed differently since there are \n",
    "                    # overlapping columns in breath and tbfvl tables\n",
    "                    feature['spx_'.format(str(i))] = float_feature(\n",
    "                        spx[spx_col].tolist()\n",
    "                    )\n",
    "                    feature['spx_{}_bool'.format(str(i))] = float_feature(\n",
    "                        spx['{}_bool'.format(spx_col)].astype(float).tolist()\n",
    "                    )\n",
    "                    i += 1\n",
    "\n",
    "                # process trial outcome\n",
    "                if trial_accepted_label == 'Accepted':\n",
    "                    feature['trial_outcome'] = int64_feature([1])\n",
    "                else:\n",
    "                    feature['trial_outcome'] = int64_feature([0])\n",
    "\n",
    "                qc_grade_label_dict = {\n",
    "                    'A/B': [1, 0, 0, 0, 0, 0],\n",
    "                    'C': [0, 1, 0, 0, 0, 0],\n",
    "                    'D': [0, 0, 1, 0, 0, 0],\n",
    "                    'E': [0, 0, 0, 1, 0, 0],\n",
    "                    'F': [0, 0, 0, 0, 1, 0],\n",
    "                    'N/A': [0, 0, 0, 0, 0, 1]\n",
    "                }\n",
    "                feature['grade'] = int64_feature(\n",
    "                    qc_grade_label_dict[qc_grade_label]\n",
    "                )\n",
    "                \n",
    "                example = tf.train.Example(\n",
    "                    features=tf.train.Features(feature=feature)\n",
    "                )\n",
    "                writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have sucessfully converted our data to the TFRecord format. We need to create some functions to decode the TFRecord data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord_fn(example):\n",
    "    \"\"\"Parse TFRecord\n",
    "\n",
    "    TFRecords contain a sequence of records. We want to process a single record\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    example : serialized Example\n",
    "        TFRecord data to be processed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dict mapping feature keys to Tensor and SparseTensor values. \n",
    "    \"\"\"\n",
    "    data_description = {}\n",
    "    for i in range(1, len(BREATH)+1):\n",
    "        data_description[\n",
    "            'breath_{}'.format(str(i))\n",
    "        ] = tf.io.FixedLenFeature([187], tf.float32)\n",
    "        data_description[\n",
    "            'breath_{}_bool'.format(str(i))\n",
    "        ] = tf.io.FixedLenFeature([187], tf.float32)\n",
    "    for i in range(1, len(TBFVL)+1):\n",
    "        data_description[\n",
    "            'tbfvl_{}'.format(str(i))\n",
    "        ] = tf.io.FixedLenFeature([203], tf.float32)\n",
    "        data_description[\n",
    "            'tbfvl_{}_bool'.format(str(i))\n",
    "        ] = tf.io.FixedLenFeature([203], tf.float32)\n",
    "    for i in range(1, len(SPX)+1):\n",
    "        data_description[\n",
    "            'spx_'.format(str(i))\n",
    "        ] = tf.io.FixedLenFeature([1], tf.float32)\n",
    "        data_description[\n",
    "            'spx_{}_bool'.format(str(i))\n",
    "        ] = tf.io.FixedLenFeature([1], tf.float32)\n",
    "    for screenshot in ['o2', 'co2', 'n2', 'flow', 'volume']:\n",
    "        data_description[\n",
    "            screenshot\n",
    "        ] = tf.io.FixedLenFeature([5748], tf.float32)\n",
    "        \n",
    "    data_description['trial_outcome'] = tf.io.FixedLenFeature([1], tf.int64)\n",
    "    data_description['grade'] = tf.io.FixedLenFeature([6], tf.int64)\n",
    "\n",
    "    example = tf.io.parse_single_example(example, data_description)\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "def prepare_sample(features):\n",
    "    \"\"\"Modify the TFRecord data\n",
    "\n",
    "    Change the column names and modify the data type\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : dict\n",
    "        A dict mapping feature keys to Tensor and SparseTensor values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    input_dict : dict\n",
    "        Processed data used for the features\n",
    "    output_dict : dict\n",
    "        Processed data used for the labels\n",
    "    \"\"\"\n",
    "    input_dict = {}\n",
    "    \n",
    "    table_dict = {\n",
    "        'breath_input': ['breath_', len(BREATH)+1],\n",
    "        'tbfvl_input': ['tbfvl_', len(TBFVL)+1],\n",
    "    }\n",
    "    for key, val in table_dict.items():\n",
    "        table_list = []\n",
    "        for i in range(1, val[1]):\n",
    "            table_list.append(features[val[0] + str(i)])\n",
    "            table_list.append(features['{}{}_bool'.format(val[0], str(i))])\n",
    "        input_dict[key] = tf.stack(table_list, axis=1)\n",
    "\n",
    "    #spx must be processed differently since there are overlapping column names\n",
    "    table_list = []\n",
    "    for i in range(1, len(SPX)+1):\n",
    "        table_list.append(features['spx_{}'.format(str(i))])\n",
    "        table_list.append(features['spx_{}_bool'.format(str(i))])\n",
    "    input_dict['spx_input'] = tf.stack(table_list, axis=1)\n",
    "\n",
    "    for screenshot in ['o2', 'co2', 'n2', 'flow', 'volume']:\n",
    "        input_dict['{}_input'.format(screenshot)] = tf.cast(\n",
    "            features[screenshot], tf.float32\n",
    "        )\n",
    "\n",
    "    output_dict = {}\n",
    "    output_dict['trial_outcome'] = tf.cast(features['trial_outcome'], tf.int32)\n",
    "    output_dict['grade'] = tf.cast(features['grade'], tf.int32)\n",
    "\n",
    "    return input_dict, output_dict\n",
    "\n",
    "def get_dataset(filenames, batch_size):\n",
    "    dataset = (\n",
    "        tf.data.TFRecordDataset(filenames, num_parallel_reads=tf.data.AUTOTUNE)\n",
    "        .map(parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .map(prepare_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .shuffle(batch_size * 10)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "        .repeat()\n",
    "        .shuffle(buffer_size=1000, reshuffle_each_iteration=True)\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.v: Model Training<a name=\"section-2v-model-training\"></a>\n",
    "We have our data in a usable format. We can now train a model using the data. The goal of this section was to train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2.v.a: Multi-Head CNN-LSTM Model<a name=\"section-2va-multi-head-cnn-lstm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The model architecture is highlighted in Figure 3. Since the breath and TBFVL table contained time series data, they were ingested using a bidirectional LSTM layer. In contrast, the SPX data did not contain time series data so it was processed with a Dense and Dropout layer. The raw MBW signals were ingested based off a [multi-head CNN-LSTM](https://www.sciencedirect.com/science/article/abs/pii/S0925231219309877) architecture. The signals are ingested using a multi-head structure that uses a one-dimensional CNN layer to process the individual time series data by extracting convoluted features. The separate CNNs are concatenated before being ingested into an bidirectonal LSTM layer. It was theorized they separate channels would be more successful in capturing the significant features of each time series and ultimately producing a better prediction.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"assets/model.png\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  Figure 3. Architecture of TensorFlow model.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    \"\"\"Model summary\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hp : keras.HyperParameters\n",
    "        Hyperparameter used to train the model instance\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    keras.Model\n",
    "        Compiled model incorporating hyperparameters\n",
    "    \"\"\"\n",
    "    breath_input = keras.Input(shape=(187, 56), name=\"breath_input\")  \n",
    "    tbfvl_input = keras.Input(shape=(203, 86), name=\"tbfvl_input\")\n",
    "    spx_input = keras.Input(shape=(1, 92), name=\"spx_input\")\n",
    "    o2_input = keras.Input(shape=(5748, 1), name=\"o2_input\")\n",
    "    co2_input = keras.Input(shape=(5748, 1), name=\"co2_input\")\n",
    "    n2_input = keras.Input(shape=(5748, 1), name=\"n2_input\")\n",
    "    flow_input = keras.Input(shape=(5748, 1), name=\"flow_input\")\n",
    "    volume_input = keras.Input(shape=(5748, 1), name=\"volume_input\")\n",
    "\n",
    "    screenshot_list = []\n",
    "    hp_kernal_pool = hp.Int('kernal_pool', min_value=3, max_value=33, step=5)\n",
    " \n",
    "    stride_pool = 1\n",
    "    stride_conv1d = 2\n",
    "\n",
    "    for ss_input in [o2_input, co2_input, n2_input, flow_input, volume_input]:\n",
    "        conv_layer_1 = keras.layers.Conv1D(\n",
    "            filters=512, kernel_size=hp_kernal_pool, activation='relu', \n",
    "            input_shape=(5748, 1), padding='valid', strides=stride_conv1d\n",
    "        )(ss_input)\n",
    "        pooling_layer_1 = keras.layers.MaxPooling1D(\n",
    "            pool_size=hp_kernal_pool, padding='same', strides=stride_pool\n",
    "        )(conv_layer_1)\n",
    "\n",
    "        conv_layer_2 = keras.layers.Conv1D(\n",
    "            filters=128, kernel_size=hp_kernal_pool, activation='relu', \n",
    "            input_shape=(2873, 512), padding='valid', strides=stride_conv1d\n",
    "        )(pooling_layer_1)\n",
    "        pooling_layer_2 = keras.layers.MaxPooling1D(\n",
    "            pool_size=hp_kernal_pool, padding='same', strides=stride_pool\n",
    "        )(conv_layer_2)\n",
    "\n",
    "        conv_layer_3 = keras.layers.Conv1D(\n",
    "            filters=64, kernel_size=hp_kernal_pool, activation='relu', \n",
    "            input_shape=(1436, 128), padding='valid', strides=stride_conv1d\n",
    "        )(pooling_layer_2)\n",
    "        pooling_layer_3 = keras.layers.MaxPooling1D(\n",
    "            pool_size=hp_kernal_pool, padding='same', strides=stride_pool\n",
    "        )(conv_layer_3)\n",
    "\n",
    "        conv_layer_4 = keras.layers.Conv1D(\n",
    "            filters=32, kernel_size=hp_kernal_pool, activation='relu', \n",
    "            input_shape=(717, 64), padding='valid', strides=stride_conv1d\n",
    "        )(pooling_layer_3)\n",
    "        pooling_layer_4 = keras.layers.MaxPooling1D(\n",
    "            pool_size=hp_kernal_pool, padding='same', strides=stride_pool\n",
    "        )(conv_layer_4)\n",
    "\n",
    "        screenshot_list.append(pooling_layer_4)\n",
    "\n",
    "    # combine all screen shots\n",
    "    screenshot = keras.layers.concatenate(screenshot_list)\n",
    "\n",
    "    # mask layers\n",
    "    breath_mask = keras.layers.Masking()(breath_input)\n",
    "    tbfvl_mask = keras.layers.Masking()(tbfvl_input)\n",
    "    screenshot_mask = keras.layers.Masking()(screenshot)\n",
    "\n",
    "    # LSTM for time series\n",
    "    hp_units_1 = hp.Int('units_1', min_value=64, max_value=1024, step=128)\n",
    "    breath_features = keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(units=hp_units_1, input_shape=[187, 56])\n",
    "    )(breath_mask)\n",
    "    tbfvl_features = keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(units=hp_units_1, input_shape=[203, 86])\n",
    "    )(tbfvl_mask)\n",
    "    screenshot_features = keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(units=hp_units_1, )\n",
    "    )(screenshot_mask)\n",
    "    \n",
    "    # dense layer to non-time series data\n",
    "    hp_units_2 = hp.Int('units_2', min_value=64, max_value=1024, step=128)\n",
    "    spx_features = keras.layers.Dense(\n",
    "        units=hp_units_2, activation='relu'\n",
    "    )(spx_input)\n",
    "\n",
    "    # individual drop out layers\n",
    "    hp_rate_1 = hp.Float('rate_1', min_value=0, max_value=0.8, step=0.2)\n",
    "    breath_features = keras.layers.Dropout(rate=hp_rate_1)(breath_features)\n",
    "    hp_rate_2 = hp.Float('rate_2', min_value=0, max_value=0.4, step=0.2)\n",
    "    tbfvl_features = keras.layers.Dropout(rate=hp_rate_2)(tbfvl_features)\n",
    "    hp_rate_3 = hp.Float('rate_3', min_value=0, max_value=0.8, step=0.2)\n",
    "    screenshot_features = keras.layers.Dropout(rate=hp_rate_3)(screenshot_features)\n",
    "    hp_rate_4 = hp.Float('rate_4', min_value=0, max_value=0.4, step=0.2)\n",
    "    spx_features = keras.layers.Dropout(rate=hp_rate_4)(spx_features)\n",
    "    \n",
    "    spx_features = keras.layers.Flatten()(spx_features)\n",
    "\n",
    "    total_features = keras.layers.concatenate(\n",
    "        [breath_features, tbfvl_features, screenshot_features, spx_features]\n",
    "    )\n",
    "\n",
    "    total_features = keras.layers.Dense(\n",
    "        units=1024, activation='relu', kernel_initializer='he_normal'\n",
    "    )(total_features)\n",
    "\n",
    "\n",
    "    trial_outcome = keras.layers.Dense(\n",
    "        1, activation='sigmoid', name='trial_outcome'\n",
    "    )(total_features)\n",
    "    grade = keras.layers.Dense(\n",
    "        6, activation='sigmoid', name='grade'\n",
    "    )(total_features)\n",
    "\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=[\n",
    "            breath_input, tbfvl_input, spx_input,\n",
    "            o2_input, co2_input, n2_input, flow_input, volume_input\n",
    "        ],\n",
    "        outputs=[trial_outcome, grade],\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=[\n",
    "            keras.losses.BinaryCrossentropy(), \n",
    "            keras.losses.CategoricalCrossentropy()\n",
    "        ],\n",
    "        optimizer='adam',\n",
    "        metrics=['acc']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2.v.b: Hyperparameter Tuning<a name=\"section-2vb-hyperparameter-tuning\"></a>\n",
    "Once we have a model, we can use the Keras tuner to determine the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ../models\\baseline_BO/tuner\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from ../models\\baseline_BO/tuner\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "train_filenames = tf.io.gfile.glob('{}/train/*.tfrec'.format(PROCESSED_PATH))\n",
    "validation_filenames = tf.io.gfile.glob(\n",
    "    '{}/validate/*.tfrec'.format(PROCESSED_PATH)\n",
    ")\n",
    "batch_size = 32\n",
    "steps_per_epoch = 300\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    model_builder,\n",
    "    objective=kt.Objective('val_trial_outcome_acc', direction='max'),\n",
    "    max_trials=7,\n",
    "    executions_per_trial=1,\n",
    "    directory='../models',\n",
    "    project_name='baseline_BO/tuner',\n",
    "    overwrite=False\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    get_dataset(train_filenames, batch_size),\n",
    "    epochs=3,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=get_dataset(validation_filenames, batch_size), \n",
    "    validation_steps=51\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the best hyperparameters according to the Keras tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernal_pool: 3\n",
      "units_1: 960\n",
      "units_2: 960\n",
      "rate_1: 0.0\n",
      "rate_2: 0.0\n",
      "rate_3: 0.8\n",
      "rate_4: 0.0\n"
     ]
    }
   ],
   "source": [
    "for tuned_value in [\n",
    "    'kernal_pool', 'units_1', 'units_2', 'rate_1', 'rate_2', 'rate_3', 'rate_4'\n",
    "]:\n",
    "    print(\n",
    "        '{}: {}'.format(\n",
    "            tuned_value, \n",
    "            str(tuner.get_best_hyperparameters(num_trials=1)[0][tuned_value])\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2.v.c: Retrain Model<a name=\"section-2vc-retrain-model\"></a>\n",
    "It is best practice to retrain the model with the best hyperparmeters to get a final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = \"../models/baseline_BO/checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_grade_loss', patience=3\n",
    ")\n",
    "# create model checkpoints incase model stopped unexpectedly\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=MODELS_PATH,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_trial_outcome_acc',\n",
    "    mode='max',\n",
    "    save_best_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "300/300 [==============================] - ETA: 0s - loss: 1.5871 - trial_outcome_loss: 0.4416 - grade_loss: 1.1454 - trial_outcome_acc: 0.7798 - grade_acc: 0.5543 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 15458s 50s/step - loss: 1.5871 - trial_outcome_loss: 0.4416 - grade_loss: 1.1454 - trial_outcome_acc: 0.7798 - grade_acc: 0.5543 - val_loss: 1.4706 - val_trial_outcome_loss: 0.3948 - val_grade_loss: 1.0758 - val_trial_outcome_acc: 0.8223 - val_grade_acc: 0.5919\n",
      "Epoch 2/25\n",
      "300/300 [==============================] - ETA: 0s - loss: 1.3785 - trial_outcome_loss: 0.3942 - grade_loss: 0.9842 - trial_outcome_acc: 0.8033 - grade_acc: 0.6102  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 20380s 68s/step - loss: 1.3785 - trial_outcome_loss: 0.3942 - grade_loss: 0.9842 - trial_outcome_acc: 0.8033 - grade_acc: 0.6102 - val_loss: 1.3079 - val_trial_outcome_loss: 0.3551 - val_grade_loss: 0.9528 - val_trial_outcome_acc: 0.8221 - val_grade_acc: 0.6316\n",
      "Epoch 3/25\n",
      "300/300 [==============================] - ETA: 0s - loss: 1.2967 - trial_outcome_loss: 0.3738 - grade_loss: 0.9229 - trial_outcome_acc: 0.8216 - grade_acc: 0.6383  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 27032s 90s/step - loss: 1.2967 - trial_outcome_loss: 0.3738 - grade_loss: 0.9229 - trial_outcome_acc: 0.8216 - grade_acc: 0.6383 - val_loss: 1.4149 - val_trial_outcome_loss: 0.3875 - val_grade_loss: 1.0274 - val_trial_outcome_acc: 0.7894 - val_grade_acc: 0.6024\n",
      "Epoch 4/25\n",
      "300/300 [==============================] - ETA: 0s - loss: 1.2362 - trial_outcome_loss: 0.3611 - grade_loss: 0.8752 - trial_outcome_acc: 0.8268 - grade_acc: 0.6627  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 34240s 114s/step - loss: 1.2362 - trial_outcome_loss: 0.3611 - grade_loss: 0.8752 - trial_outcome_acc: 0.8268 - grade_acc: 0.6627 - val_loss: 1.3076 - val_trial_outcome_loss: 0.3592 - val_grade_loss: 0.9483 - val_trial_outcome_acc: 0.8315 - val_grade_acc: 0.6305\n",
      "Epoch 5/25\n",
      "300/300 [==============================] - ETA: 0s - loss: 1.1941 - trial_outcome_loss: 0.3467 - grade_loss: 0.8475 - trial_outcome_acc: 0.8333 - grade_acc: 0.6696  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 36297s 121s/step - loss: 1.1941 - trial_outcome_loss: 0.3467 - grade_loss: 0.8475 - trial_outcome_acc: 0.8333 - grade_acc: 0.6696 - val_loss: 1.3248 - val_trial_outcome_loss: 0.3588 - val_grade_loss: 0.9660 - val_trial_outcome_acc: 0.8109 - val_grade_acc: 0.6267\n",
      "Epoch 6/25\n",
      "300/300 [==============================] - ETA: 0s - loss: 1.1439 - trial_outcome_loss: 0.3343 - grade_loss: 0.8096 - trial_outcome_acc: 0.8418 - grade_acc: 0.6876  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 40166s 134s/step - loss: 1.1439 - trial_outcome_loss: 0.3343 - grade_loss: 0.8096 - trial_outcome_acc: 0.8418 - grade_acc: 0.6876 - val_loss: 1.2938 - val_trial_outcome_loss: 0.3521 - val_grade_loss: 0.9417 - val_trial_outcome_acc: 0.8252 - val_grade_acc: 0.6336\n",
      "Epoch 7/25\n",
      "300/300 [==============================] - ETA: 0s - loss: 1.1100 - trial_outcome_loss: 0.3188 - grade_loss: 0.7913 - trial_outcome_acc: 0.8563 - grade_acc: 0.6919  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 42616s 142s/step - loss: 1.1100 - trial_outcome_loss: 0.3188 - grade_loss: 0.7913 - trial_outcome_acc: 0.8563 - grade_acc: 0.6919 - val_loss: 1.2689 - val_trial_outcome_loss: 0.3501 - val_grade_loss: 0.9188 - val_trial_outcome_acc: 0.8254 - val_grade_acc: 0.6262\n",
      "Epoch 8/25\n",
      "300/300 [==============================] - ETA: 0s - loss: 1.0951 - trial_outcome_loss: 0.3192 - grade_loss: 0.7760 - trial_outcome_acc: 0.8510 - grade_acc: 0.6983  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 42908s 142s/step - loss: 1.0951 - trial_outcome_loss: 0.3192 - grade_loss: 0.7760 - trial_outcome_acc: 0.8510 - grade_acc: 0.6983 - val_loss: 1.2957 - val_trial_outcome_loss: 0.3495 - val_grade_loss: 0.9462 - val_trial_outcome_acc: 0.8208 - val_grade_acc: 0.6186\n",
      "Epoch 9/25\n",
      "300/300 [==============================] - ETA: 0s - loss: 1.0669 - trial_outcome_loss: 0.3134 - grade_loss: 0.7534 - trial_outcome_acc: 0.8571 - grade_acc: 0.7050  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 41918s 139s/step - loss: 1.0669 - trial_outcome_loss: 0.3134 - grade_loss: 0.7534 - trial_outcome_acc: 0.8571 - grade_acc: 0.7050 - val_loss: 1.3092 - val_trial_outcome_loss: 0.3541 - val_grade_loss: 0.9551 - val_trial_outcome_acc: 0.8352 - val_grade_acc: 0.6471\n",
      "Epoch 10/25\n",
      "300/300 [==============================] - ETA: 0s - loss: 1.0134 - trial_outcome_loss: 0.2941 - grade_loss: 0.7193 - trial_outcome_acc: 0.8679 - grade_acc: 0.7189  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO\\checkpoints\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 44116s 147s/step - loss: 1.0134 - trial_outcome_loss: 0.2941 - grade_loss: 0.7193 - trial_outcome_acc: 0.8679 - grade_acc: 0.7189 - val_loss: 1.4881 - val_trial_outcome_loss: 0.4207 - val_grade_loss: 1.0673 - val_trial_outcome_acc: 0.8076 - val_grade_acc: 0.6244\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Retrain the model\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    x=get_dataset(train_filenames, batch_size), \n",
    "    epochs=25, \n",
    "    validation_data=get_dataset(validation_filenames, batch_size),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=[early_stop, model_checkpoint_callback], \n",
    "    validation_steps=51\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was scheduled to run for 25 epochs but ran for only 10. This is because an \"early stopping\" callback was defined that monitored the validation grade loss. The \"patience\" was set to 3 meaning the lowerest validation grade loss was achived in the 7th epoch (0.9188) but was allowed to train for 3 additional epochs to confirm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Results<a name=\"section-3-results\"></a>\n",
    "Once we have a final model, we can load the test files to get our model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filenames = tf.io.gfile.glob(f\"{PROCESSED_PATH}/test/*.tfrec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 3291s 43s/step - loss: 1.4068 - trial_outcome_loss: 0.3917 - grade_loss: 1.0151 - trial_outcome_acc: 0.8422 - grade_acc: 0.6539\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(\n",
    "    x=get_dataset(test_filenames, batch_size),\n",
    "    steps=57,\n",
    "    return_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  1.4068259000778198\n",
      "trial_outcome_loss :  0.39172881841659546\n",
      "grade_loss :  1.0150971412658691\n",
      "trial_outcome_acc :  0.8422222137451172\n",
      "grade_acc :  0.6538888812065125\n"
     ]
    }
   ],
   "source": [
    "for key, value in results.items():\n",
    "    print(key, ': ', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO/final_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/baseline_BO/final_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('../models/baseline_BO/final_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Discussion<a name=\"section-4-discussion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Although the 84.2% accuracy of the model is respectable, there is still room for improvement. Techniques such as data augmentation or feature engineering may have resulted in a higher accruacy. Additionally, modifying the model's training parameters such as training for longer, separately tuning the hyperparameters of each layer, or using a wegithed Kappy loss for the ordinal grade labels could have had a significant impact. Ultimately, it was decided to cease development of this model as it was believed that a logistic regression would be a more appropriate methodology for this use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "020e0c309d88da7edcb1f4dd199dd7cedefd9e876487b268c4a8f57aa874461b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
